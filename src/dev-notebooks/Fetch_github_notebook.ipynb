{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QFLdAWNGJlP3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "from typing import Optional, List, Tuple, Literal, Dict\n",
        "from urllib.parse import urlparse\n",
        "from datetime import datetime\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "load_dotenv()  # loads OPENAI_API_KEY if you store it in .env\n",
        "assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not set\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data models (public-repo friendly)\n",
        "\n",
        "Code to fetch github prs and create pydantic models to access required features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class GitUser(BaseModel):\n",
        "    login: str\n",
        "    id: int\n",
        "    node_id: Optional[str] = None\n",
        "    avatar_url: Optional[str] = None\n",
        "    gravatar_id: Optional[str] = None\n",
        "    url: Optional[str] = None\n",
        "    html_url: Optional[str] = None\n",
        "    type: Optional[str] = None\n",
        "\n",
        "class Repository(BaseModel):\n",
        "    url: Optional[str] = None\n",
        "    svn_url: Optional[str] = None\n",
        "\n",
        "\n",
        "class PRRef(BaseModel):\n",
        "    sha: str\n",
        "    ref: str\n",
        "    label: Optional[str] = None\n",
        "    repo: Optional[Repository] = None\n",
        "\n",
        "class PRDetails(BaseModel):\n",
        "    title: str\n",
        "    body: Optional[str] = None\n",
        "    user: GitUser\n",
        "    created_at: datetime\n",
        "    updated_at: datetime\n",
        "    merged: bool\n",
        "    mergeable: Optional[bool] = None\n",
        "    commits: int\n",
        "    additions: int\n",
        "    deletions: int\n",
        "    changed_files: int\n",
        "    head: PRRef\n",
        "    base: PRRef\n",
        "\n",
        "class PRFile(BaseModel):\n",
        "    sha: str\n",
        "    filename: str\n",
        "    status: Literal[\"added\",\"modified\",\"removed\",\"renamed\",\"copied\",\"changed\",\"unchanged\"]\n",
        "    additions: int\n",
        "    deletions: int\n",
        "    changes: int\n",
        "    blob_url: Optional[str] = None\n",
        "    raw_url: Optional[str] = None\n",
        "    contents_url: Optional[str] = None\n",
        "    patch: Optional[str] = None\n",
        "    previous_filename: Optional[str] = None\n",
        "    original_file_url: Optional[str] = None\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Github Connector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GithubAPIConnector:\n",
        "    def __init__(self, pr_url: str):\n",
        "        parts = [p for p in urlparse(pr_url).path.split(\"/\") if p]\n",
        "        if len(parts) < 4 or parts[2] != \"pull\":\n",
        "            raise ValueError(\n",
        "                f\"Expected https://github.com/<owner>/<repo>/pull/<num>, got: {pr_url}\"\n",
        "            )\n",
        "\n",
        "        # STORE these as attributes\n",
        "        self.owner = parts[0]\n",
        "        self.repo = parts[1]\n",
        "        self.pr_number = parts[3]\n",
        "\n",
        "        self.api_pr_url = (\n",
        "            f\"https://api.github.com/repos/{self.owner}/{self.repo}/pulls/{self.pr_number}\"\n",
        "        )\n",
        "        self.files_url = (\n",
        "            f\"https://api.github.com/repos/{self.owner}/{self.repo}/pulls/{self.pr_number}/files\"\n",
        "        )\n",
        "\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update(\n",
        "            {\"Accept\": \"application/vnd.github+json\"}\n",
        "        )\n",
        "\n",
        "    def get_pr_details(self) -> PRDetails:\n",
        "        response = requests.get(self.api_pr_url)\n",
        "        response.raise_for_status()\n",
        "        return PRDetails.model_validate(response.json())\n",
        "\n",
        "    def get_pr_files(self) -> list[PRFile]:\n",
        "        response = requests.get(self.files_url)\n",
        "        response.raise_for_status()\n",
        "        return [PRFile.model_validate(file) for file in response.json()]\n",
        "\n",
        "class GithubPRFilesFetcher():\n",
        "    def __init__(self, pr_url):\n",
        "        self.connector = GithubAPIConnector(pr_url)\n",
        "\n",
        "    def fetch_pr(self) -> tuple[PRDetails, list[PRFile]]:\n",
        "        pr = self.connector.get_pr_details()\n",
        "        files = self.connector.get_pr_files()\n",
        "\n",
        "        base_sha = pr.base.sha\n",
        "        owner, repo = self.connector.owner, self.connector.repo\n",
        "\n",
        "        for f in files:\n",
        "            if f.status in (\"modified\", \"renamed\"):\n",
        "                f.original_file_url = (\n",
        "                    f\"https://raw.githubusercontent.com/{owner}/{repo}/{base_sha}/{f.filename}\"\n",
        "                )\n",
        "\n",
        "        return pr, files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pull_request_url = 'https://github.com/topoteretes/cognee/pull/1851'\n",
        "# pr_fetcher = GithubPRFilesFetcher(pr_url=pull_request_url)\n",
        "# files = pr_fetcher.fetch_pr_files()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Context enrichment: parse hunks → fetch targeted windows from actual file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parse “added-side” hunks from patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "HUNK_RE = re.compile(r\"@@ -\\d+(?:,\\d+)? \\+(\\d+)(?:,(\\d+))? @@\")\n",
        "\n",
        "def parse_added_hunks(patch: Optional[str]) -> List[Tuple[int, int]]:\n",
        "    \"\"\"\n",
        "    Returns list of (start_line, length) for the + (new) side hunks.\n",
        "    \"\"\"\n",
        "    if not patch:\n",
        "        return []\n",
        "    hunks = []\n",
        "    for m in HUNK_RE.finditer(patch):\n",
        "        start = int(m.group(1))\n",
        "        length = int(m.group(2) or \"1\")\n",
        "        hunks.append((start, length))\n",
        "    return hunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch file content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A simple tool to fetch text from a URL\n",
        "import requests\n",
        "from typing import Optional\n",
        "\n",
        "def fetch_text(url: Optional[str], timeout: int = 30) -> Optional[str]:\n",
        "    if not url:\n",
        "        return None\n",
        "    r = requests.get(url, timeout=timeout)\n",
        "    if r.status_code == 404:\n",
        "        return None\n",
        "    r.raise_for_status()\n",
        "    return r.text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract windows around hunks and merge overlaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_ranges(ranges: List[Tuple[int,int]]) -> List[Tuple[int,int]]:\n",
        "    \"\"\"Merge overlapping [start,end] inclusive ranges (1-indexed).\"\"\"\n",
        "    if not ranges:\n",
        "        return []\n",
        "    ranges = sorted(ranges)\n",
        "    merged = [ranges[0]]\n",
        "    for s,e in ranges[1:]:\n",
        "        ps,pe = merged[-1]\n",
        "        if s <= pe + 1:\n",
        "            merged[-1] = (ps, max(pe, e))\n",
        "        else:\n",
        "            merged.append((s,e))\n",
        "    return merged\n",
        "\n",
        "def extract_windows_from_hunks(file_text: str, hunks: List[Tuple[int,int]], padding: int = 30, max_total_lines: int = 260) -> str:\n",
        "    lines = file_text.splitlines()\n",
        "    # convert each hunk into [start, end] range with padding\n",
        "    ranges = []\n",
        "    for start, length in hunks:\n",
        "        s = max(1, start - padding)\n",
        "        e = min(len(lines), start + length + padding)\n",
        "        ranges.append((s,e))\n",
        "\n",
        "    ranges = merge_ranges(ranges)\n",
        "\n",
        "    snippets = []\n",
        "    used = 0\n",
        "    for s,e in ranges:\n",
        "        if used >= max_total_lines:\n",
        "            break\n",
        "        take_s = s\n",
        "        take_e = min(e, s + (max_total_lines - used) - 1)\n",
        "        snippet = \"\\n\".join(lines[take_s-1:take_e])\n",
        "        snippets.append(f\"[lines {take_s}-{take_e}]\\n{snippet}\")\n",
        "        used += (take_e - take_s + 1)\n",
        "\n",
        "    return \"\\n\\n\".join(snippets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the “review bundle” (patch + targeted file context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_pr_header(pr: PRDetails) -> str:\n",
        "    return \"\\n\".join([\n",
        "        f\"PR TITLE: {pr.title}\",\n",
        "        f\"PR AUTHOR: {pr.user.login} ({pr.user.type})\",\n",
        "        f\"CREATED: {pr.created_at.isoformat()} | UPDATED: {pr.updated_at.isoformat()}\",\n",
        "        f\"COMMITS: {pr.commits} | +{pr.additions} -{pr.deletions} | FILES: {pr.changed_files}\",\n",
        "        \"\",\n",
        "        \"PR DESCRIPTION:\",\n",
        "        (pr.body or \"\").strip()[:2000],\n",
        "        \"\",\n",
        "    ])\n",
        "\n",
        "def build_patch_only_bundle(files: List[PRFile], max_patch_chars: int = 6000) -> str:\n",
        "    chunks = []\n",
        "    for f in files:\n",
        "        patch = (f.patch or \"\")\n",
        "        patch = patch[:max_patch_chars] if patch else \"\"\n",
        "        chunks.append(\"\\n\".join([\n",
        "            f\"FILE: {f.filename} | status={f.status} | +{f.additions} -{f.deletions} | changes={f.changes}\",\n",
        "            \"PATCH:\",\n",
        "            patch if patch else \"[No patch provided by GitHub API]\",\n",
        "            \"-\"*60\n",
        "        ]))\n",
        "    return \"\\n\".join(chunks)\n",
        "\n",
        "def build_enriched_bundle(files: List[PRFile], focus_files: List[str], padding: int = 30) -> str:\n",
        "    focus_set = set(focus_files)\n",
        "    chunks = []\n",
        "    for f in files:\n",
        "        if f.filename not in focus_set:\n",
        "            continue\n",
        "\n",
        "        head_text = fetch_text(f.raw_url)\n",
        "        hunks = parse_added_hunks(f.patch)\n",
        "        context = \"\"\n",
        "        if head_text and hunks:\n",
        "            context = extract_windows_from_hunks(head_text, hunks, padding=padding, max_total_lines=260)\n",
        "        elif head_text:\n",
        "            # fallback: top slice\n",
        "            lines = head_text.splitlines()\n",
        "            context = \"\\n\".join(lines[:140]) + (\"\\n...\\n\" if len(lines) > 160 else \"\")\n",
        "\n",
        "        chunks.append(\"\\n\".join([\n",
        "            f\"FILE: {f.filename} | status={f.status} | +{f.additions} -{f.deletions} | changes={f.changes}\",\n",
        "            \"TARGETED HEAD CONTEXT:\",\n",
        "            context if context else \"[Could not fetch/extract head context]\",\n",
        "            \"-\"*60\n",
        "        ]))\n",
        "    return \"\\n\".join(chunks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agent schemas (structured)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "Severity = Literal[\"P0\", \"P1\", \"P2\"]\n",
        "Category = Literal[\"correctness\", \"security\", \"performance\", \"maintainability\"]\n",
        "\n",
        "class Finding(BaseModel):\n",
        "    severity: Severity\n",
        "    category: Category\n",
        "    file: Optional[str] = None\n",
        "    line_range: Optional[str] = None\n",
        "    title: str\n",
        "    description: str\n",
        "    recommendation: str\n",
        "    confidence: float  # 0.0 - 1.0\n",
        "\n",
        "class AgentFindings(BaseModel):\n",
        "    findings: List[Finding]\n",
        "\n",
        "\n",
        "class TriageResult(BaseModel):\n",
        "    summary: str\n",
        "    risk: Literal[\"low\", \"medium\", \"high\"]\n",
        "    focus_files: List[str]\n",
        "    questions_for_author: List[str] = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM agents (Triage + Correctness + Security)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
        "\n",
        "# Triage Agent\n",
        "def triage_agent(pr_header: str, patch_bundle: str, llm) -> TriageResult:\n",
        "    system = \"\"\"You are a senior tech lead doing PR triage.\n",
        "Pick a small set of focus files (max 5) that are most important/risky to review deeply.\n",
        "Only use information in the PR header and patches provided.\n",
        "\"\"\"\n",
        "\n",
        "    user = f\"\"\"Return JSON matching this schema:\n",
        "{TriageResult.model_json_schema()}\n",
        "\n",
        "PR HEADER:\n",
        "{pr_header}\n",
        "\n",
        "PATCHES:\n",
        "{patch_bundle}\n",
        "\"\"\"\n",
        "    structured = llm.with_structured_output(TriageResult)\n",
        "    return structured.invoke([SystemMessage(content=system), HumanMessage(content=user)])\n",
        "\n",
        "# Correctness Agent\n",
        "def correctness_agent(pr_header: str, enriched_bundle: str, llm) -> List[Finding]:\n",
        "    system = \"\"\"You are a correctness-focused code reviewer.\n",
        "Find concrete, high-signal issues grounded in the provided context.\n",
        "Prefer fewer, higher impact findings. If unsure, lower confidence.\n",
        "\"\"\"\n",
        "    user = f\"\"\"Return JSON matching this schema:\n",
        "{AgentFindings.model_json_schema()}\n",
        "\n",
        "PR HEADER:\n",
        "{pr_header}\n",
        "\n",
        "ENRICHED CONTEXT (targeted file windows):\n",
        "{enriched_bundle}\n",
        "\"\"\"\n",
        "    structured = llm.with_structured_output(AgentFindings)\n",
        "    return structured.invoke([SystemMessage(content=system), HumanMessage(content=user)]).findings\n",
        "\n",
        "# Security Agent\n",
        "def security_agent(pr_header: str, enriched_bundle: str, llm) -> List[Finding]:\n",
        "    system = \"\"\"You are a security-focused code reviewer.\n",
        "Look for auth/authz mistakes, secrets leakage, injection risks, unsafe logging, insecure defaults.\n",
        "Only report issues supported by the diff/context. If unsure, ask a question instead of asserting.\n",
        "\"\"\"\n",
        "    user = f\"\"\"Return JSON matching this schema:\n",
        "{AgentFindings.model_json_schema()}\n",
        "\n",
        "PR HEADER:\n",
        "{pr_header}\n",
        "\n",
        "ENRICHED CONTEXT (targeted file windows):\n",
        "{enriched_bundle}\n",
        "\"\"\"\n",
        "    structured = llm.with_structured_output(AgentFindings)\n",
        "    return structured.invoke([SystemMessage(content=system), HumanMessage(content=user)]).findings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lead reviewer: dedupe + rank + render Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dedupe_findings(findings: List[Finding]) -> List[Finding]:\n",
        "    # simple heuristic: same file + same title => same issue\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for f in findings:\n",
        "        key = (f.file or \"\", f.title.strip().lower(), f.category)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        out.append(f)\n",
        "    # sort by severity then confidence desc\n",
        "    sev_order = {\"P0\": 0, \"P1\": 1, \"P2\": 2}\n",
        "    out.sort(key=lambda x: (sev_order.get(x.severity, 9), -x.confidence))\n",
        "    return out\n",
        "\n",
        "def render_findings_md(items: List[Finding]) -> str:\n",
        "    if not items:\n",
        "        return \"_None_\"\n",
        "    lines = []\n",
        "    for f in items:\n",
        "        loc = \"\"\n",
        "        if f.file:\n",
        "            loc = f.file\n",
        "            if f.line_range:\n",
        "                loc += f\":{f.line_range}\"\n",
        "        loc = f\" — `{loc}`\" if loc else \"\"\n",
        "        lines.append(\n",
        "            f\"- **[{f.category.upper()}] {f.title}**{loc}\\n\"\n",
        "            f\"  - {f.description}\\n\"\n",
        "            f\"  - **Recommendation:** {f.recommendation}\\n\"\n",
        "            f\"  - Confidence: `{f.confidence:.2f}`\"\n",
        "        )\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def lead_reviewer(pr: PRDetails, triage: TriageResult, findings: List[Finding]) -> str:\n",
        "    findings = dedupe_findings(findings)\n",
        "    p0 = [f for f in findings if f.severity == \"P0\"]\n",
        "    p1 = [f for f in findings if f.severity == \"P1\"]\n",
        "    p2 = [f for f in findings if f.severity == \"P2\"]\n",
        "\n",
        "    q_md = \"\\n\".join([f\"- {q}\" for q in triage.questions_for_author]) if triage.questions_for_author else \"_None_\"\n",
        "\n",
        "    return f\"\"\"# SentinelReview Report\n",
        "\n",
        "## PR\n",
        "- **Title:** {pr.title}\n",
        "- **Author:** {pr.user.login} ({pr.user.type})\n",
        "- **Files Changed:** {pr.changed_files} | **Commits:** {pr.commits} | **Net:** +{pr.additions} -{pr.deletions}\n",
        "- **Triage Risk:** **{triage.risk.upper()}**\n",
        "\n",
        "## Summary\n",
        "{triage.summary}\n",
        "\n",
        "## Questions for the author\n",
        "{q_md}\n",
        "\n",
        "## Findings\n",
        "\n",
        "### P0 (Must fix)\n",
        "{render_findings_md(p0)}\n",
        "\n",
        "### P1 (Should fix)\n",
        "{render_findings_md(p1)}\n",
        "\n",
        "### P2 (Nice to have)\n",
        "{render_findings_md(p2)}\n",
        "\n",
        "## Focus files reviewed deeply\n",
        "{\", \".join(triage.focus_files) if triage.focus_files else \"_None_\"}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End-to-end run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Triage focus files: ['README.md']\n",
            "# SentinelReview Report\n",
            "\n",
            "## PR\n",
            "- **Title:** fix: Specify that SentinelReview is a Python project\n",
            "- **Author:** nikhiltyagi1991 (User)\n",
            "- **Files Changed:** 1 | **Commits:** 1 | **Net:** +2 -0\n",
            "- **Triage Risk:** **LOW**\n",
            "\n",
            "## Summary\n",
            "This PR adds a section to the README.md file to specify that SentinelReview is a Python project.\n",
            "\n",
            "## Questions for the author\n",
            "_None_\n",
            "\n",
            "## Findings\n",
            "\n",
            "### P0 (Must fix)\n",
            "_None_\n",
            "\n",
            "### P1 (Should fix)\n",
            "_None_\n",
            "\n",
            "### P2 (Nice to have)\n",
            "- **[MAINTAINABILITY] Add Python project specification section lacks detail and formatting** — `README.md:5-6`\n",
            "  - The added section '## This is a python project' in the README.md is minimal and does not provide sufficient information about the Python environment, dependencies, or setup instructions. Additionally, the section header is not capitalized consistently ('python' should be 'Python').\n",
            "  - **Recommendation:** Enhance the Python project section by including details such as Python version, dependency management (e.g., requirements.txt or Pipfile), and setup instructions. Also, capitalize 'Python' in the section header for consistency and professionalism.\n",
            "  - Confidence: `0.90`\n",
            "\n",
            "## Focus files reviewed deeply\n",
            "README.md\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pull_request_url = \"https://github.com/kiranrawat/SentinelReview/pull/1\"\n",
        "\n",
        "\n",
        "#  \"https://github.com/langchain-ai/langgraph/pulls/6641\"\n",
        "# pull_request_url = \"https://github.com/topoteretes/cognee/pull/1851\"\n",
        "\n",
        "\n",
        "fetcher = GithubPRFilesFetcher(pr_url=pull_request_url)\n",
        "pr, files = fetcher.fetch_pr()\n",
        "\n",
        "pr_header = build_pr_header(pr)\n",
        "patch_bundle = build_patch_only_bundle(files)\n",
        "\n",
        "# 1) TRIAGE decides focus files\n",
        "triage = triage_agent(pr_header, patch_bundle, llm)\n",
        "print(\"Triage focus files:\", triage.focus_files)\n",
        "\n",
        "# 2) Build enriched context only for focus files\n",
        "enriched = build_enriched_bundle(files, triage.focus_files, padding=30)\n",
        "\n",
        "# 3) Specialist agents\n",
        "findings = []\n",
        "findings += correctness_agent(pr_header, enriched, llm)\n",
        "findings += security_agent(pr_header, enriched, llm)\n",
        "\n",
        "# 4) Final report\n",
        "report_md = lead_reviewer(pr, triage, findings)\n",
        "print(report_md)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Finding(severity='P1', category='correctness', file='cognee/cli/tui/cognify_screen.py', line_range='144-169', title='Incomplete code in _cognify_async method', description=\"The _cognify_async method in CognifyTUIScreen ends abruptly with an incomplete line 'background_checkbox = sel', which is a syntax error and will cause the TUI to fail when cognify is executed.\", recommendation='Complete the method implementation by properly re-enabling the inputs and removing the incomplete line. Ensure the method ends with correct syntax and logic to re-enable UI elements after processing.', confidence=0.9),\n",
              " Finding(severity='P1', category='correctness', file='cognee/cli/tui/delete_screen.py', line_range='211-247', title='Incomplete code in _handle_delete_all method', description=\"The _handle_delete_all method in DeleteTUIScreen ends abruptly with an incomplete line 'se', which is a syntax error and will cause the TUI to fail when attempting to delete all data.\", recommendation='Complete the method implementation by properly handling the confirmation dialog and deletion logic. Remove the incomplete line and ensure the method has correct syntax and logic to confirm and perform delete all operation.', confidence=0.9),\n",
              " Finding(severity='P1', category='correctness', file='cognee/cli/tui/config_screen.py', line_range='97-104', title='Incomplete code in _load_table_data method', description=\"The _load_table_data method in ConfigTUIScreen is truncated and ends abruptly with 'for key, (_, default_val) in self.CONFI', indicating incomplete code. This will cause runtime errors when loading configuration data in the TUI.\", recommendation='Complete the method implementation to fully load configuration data into the table. Ensure the loop and all related logic are properly implemented and the method ends correctly.', confidence=0.9),\n",
              " Finding(severity='P1', category='security', file='cognee/cli/tui/config_screen.py', line_range='entire file', title='Potential Exposure of Sensitive Configuration Data in TUI Config Screen', description=\"The TUI config screen allows inline editing of configuration keys including sensitive keys such as 'llm_api_key', 'vector_db_key', and 'llm_endpoint'. These keys may contain secrets or credentials. Displaying and editing these values in a TUI without masking or additional protection risks accidental exposure of secrets to shoulder surfing or logs.\", recommendation='Implement masking or obfuscation for sensitive configuration values in the TUI. Ensure that sensitive values are not logged or displayed in plain text. Consider adding authentication or authorization checks before allowing access to sensitive configuration editing.', confidence=0.9),\n",
              " Finding(severity='P2', category='security', file='cognee/cli/tui/delete_screen.py', line_range='entire file', title='Lack of Confirmation for Bulk Delete Operations in TUI Delete Screen', description=\"The delete screen allows deletion of datasets or all user data with minimal confirmation. The 'Delete All' operation can be triggered via Ctrl+A or a button, and while there is a mention of confirmation, the code snippet is incomplete and does not clearly show robust confirmation or authorization checks before destructive actions.\", recommendation=\"Ensure that destructive operations like 'Delete All' require explicit user confirmation and proper authorization. Implement confirmation dialogs and possibly require re-authentication or additional safeguards to prevent accidental or unauthorized data deletion.\", confidence=0.8),\n",
              " Finding(severity='P2', category='security', file='cognee/infrastructure/databases/relational/sqlalchemy/SqlAlchemyAdapter.py', line_range='256-310', title='Improper Handling of Deletion Inputs in delete_entities_by_id Method', description='The method delete_entities_by_id accepts a single UUID or a list of UUIDs for deletion. It converts single UUIDs to a list internally. However, if an empty list is passed, it returns early without error. There is no explicit validation of UUID formats or authorization checks before deletion. This could lead to unintended deletions if inputs are not properly validated upstream.', recommendation='Add input validation to ensure that all UUIDs are valid before deletion. Implement authorization checks to ensure the caller has permission to delete the specified entities. Consider logging deletion operations for audit purposes.', confidence=0.7)]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "findings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'@@ -92,6 +92,7 @@ def _discover_commands() -> List[Type[SupportsCliCommand]]:\\n         (\"cognee.cli.commands.cognify_command\", \"CognifyCommand\"),\\n         (\"cognee.cli.commands.delete_command\", \"DeleteCommand\"),\\n         (\"cognee.cli.commands.config_command\", \"ConfigCommand\"),\\n+        (\"cognee.cli.commands.tui_command\", \"TuiCommand\"),\\n     ]\\n \\n     for module_path, class_name in command_modules:'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "files[0].patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
