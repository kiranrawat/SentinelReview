{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "QFLdAWNGJlP3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "from typing import Optional, List, Tuple, Literal, Dict\n",
        "from urllib.parse import urlparse\n",
        "from datetime import datetime\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "load_dotenv()  # loads OPENAI_API_KEY if you store it in .env\n",
        "assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not set\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data models (public-repo friendly)\n",
        "\n",
        "Code to fetch github prs and create pydantic models to access required features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class GitUser(BaseModel):\n",
        "    login: str\n",
        "    id: int\n",
        "    node_id: Optional[str] = None\n",
        "    avatar_url: Optional[str] = None\n",
        "    gravatar_id: Optional[str] = None\n",
        "    url: Optional[str] = None\n",
        "    html_url: Optional[str] = None\n",
        "    type: Optional[str] = None\n",
        "\n",
        "class Repository(BaseModel):\n",
        "    url: Optional[str] = None\n",
        "    svn_url: Optional[str] = None\n",
        "\n",
        "\n",
        "class PRRef(BaseModel):\n",
        "    sha: str\n",
        "    ref: str\n",
        "    label: Optional[str] = None\n",
        "    repo: Optional[Repository] = None\n",
        "\n",
        "class PRDetails(BaseModel):\n",
        "    title: str\n",
        "    body: Optional[str] = None\n",
        "    user: GitUser\n",
        "    created_at: datetime\n",
        "    updated_at: datetime\n",
        "    merged: bool\n",
        "    mergeable: Optional[bool] = None\n",
        "    commits: int\n",
        "    additions: int\n",
        "    deletions: int\n",
        "    changed_files: int\n",
        "    head: PRRef\n",
        "    base: PRRef\n",
        "\n",
        "class PRFile(BaseModel):\n",
        "    sha: str\n",
        "    filename: str\n",
        "    status: Literal[\"added\",\"modified\",\"removed\",\"renamed\",\"copied\",\"changed\",\"unchanged\"]\n",
        "    additions: int\n",
        "    deletions: int\n",
        "    changes: int\n",
        "    blob_url: Optional[str] = None\n",
        "    raw_url: Optional[str] = None\n",
        "    contents_url: Optional[str] = None\n",
        "    patch: Optional[str] = None\n",
        "    previous_filename: Optional[str] = None\n",
        "    original_file_url: Optional[str] = None\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Github Connector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GithubAPIConnector:\n",
        "    def __init__(self, pr_url: str):\n",
        "        parts = [p for p in urlparse(pr_url).path.split(\"/\") if p]\n",
        "        if len(parts) < 4 or parts[2] != \"pull\":\n",
        "            raise ValueError(\n",
        "                f\"Expected https://github.com/<owner>/<repo>/pull/<num>, got: {pr_url}\"\n",
        "            )\n",
        "\n",
        "        # STORE these as attributes\n",
        "        self.owner = parts[0]\n",
        "        self.repo = parts[1]\n",
        "        self.pr_number = parts[3]\n",
        "\n",
        "        self.api_pr_url = (\n",
        "            f\"https://api.github.com/repos/{self.owner}/{self.repo}/pulls/{self.pr_number}\"\n",
        "        )\n",
        "        self.files_url = (\n",
        "            f\"https://api.github.com/repos/{self.owner}/{self.repo}/pulls/{self.pr_number}/files\"\n",
        "        )\n",
        "\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update(\n",
        "            {\"Accept\": \"application/vnd.github+json\"}\n",
        "        )\n",
        "\n",
        "    def get_pr_details(self) -> PRDetails:\n",
        "        response = requests.get(self.api_pr_url)\n",
        "        response.raise_for_status()\n",
        "        return PRDetails.model_validate(response.json())\n",
        "\n",
        "    def get_pr_files(self) -> list[PRFile]:\n",
        "        response = requests.get(self.files_url)\n",
        "        response.raise_for_status()\n",
        "        return [PRFile.model_validate(file) for file in response.json()]\n",
        "\n",
        "class GithubPRFilesFetcher():\n",
        "    def __init__(self, pr_url):\n",
        "        self.connector = GithubAPIConnector(pr_url)\n",
        "\n",
        "    def fetch_pr(self) -> tuple[PRDetails, list[PRFile]]:\n",
        "        pr = self.connector.get_pr_details()\n",
        "        files = self.connector.get_pr_files()\n",
        "\n",
        "        base_sha = pr.base.sha\n",
        "        owner, repo = self.connector.owner, self.connector.repo\n",
        "\n",
        "        for f in files:\n",
        "            if f.status in (\"modified\", \"renamed\"):\n",
        "                f.original_file_url = (\n",
        "                    f\"https://raw.githubusercontent.com/{owner}/{repo}/{base_sha}/{f.filename}\"\n",
        "                )\n",
        "\n",
        "        return pr, files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pull_request_url = 'https://github.com/topoteretes/cognee/pull/1851'\n",
        "# pr_fetcher = GithubPRFilesFetcher(pr_url=pull_request_url)\n",
        "# files = pr_fetcher.fetch_pr_files()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Context enrichment: parse hunks → fetch targeted windows from actual file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parse “added-side” hunks from patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "HUNK_RE = re.compile(r\"@@ -\\d+(?:,\\d+)? \\+(\\d+)(?:,(\\d+))? @@\")\n",
        "\n",
        "def parse_added_hunks(patch: Optional[str]) -> List[Tuple[int, int]]:\n",
        "    \"\"\"\n",
        "    Returns list of (start_line, length) for the + (new) side hunks.\n",
        "    \"\"\"\n",
        "    if not patch:\n",
        "        return []\n",
        "    hunks = []\n",
        "    for m in HUNK_RE.finditer(patch):\n",
        "        start = int(m.group(1))\n",
        "        length = int(m.group(2) or \"1\")\n",
        "        hunks.append((start, length))\n",
        "    return hunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch file content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A simple tool to fetch text from a URL\n",
        "import requests\n",
        "from typing import Optional\n",
        "\n",
        "def fetch_text(url: Optional[str], timeout: int = 30) -> Optional[str]:\n",
        "    if not url:\n",
        "        return None\n",
        "    r = requests.get(url, timeout=timeout)\n",
        "    if r.status_code == 404:\n",
        "        return None\n",
        "    r.raise_for_status()\n",
        "    return r.text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract windows around hunks and merge overlaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_ranges(ranges: List[Tuple[int,int]]) -> List[Tuple[int,int]]:\n",
        "    \"\"\"Merge overlapping [start,end] inclusive ranges (1-indexed).\"\"\"\n",
        "    if not ranges:\n",
        "        return []\n",
        "    ranges = sorted(ranges)\n",
        "    merged = [ranges[0]]\n",
        "    for s,e in ranges[1:]:\n",
        "        ps,pe = merged[-1]\n",
        "        if s <= pe + 1:\n",
        "            merged[-1] = (ps, max(pe, e))\n",
        "        else:\n",
        "            merged.append((s,e))\n",
        "    return merged\n",
        "\n",
        "def extract_windows_from_hunks(file_text: str, hunks: List[Tuple[int,int]], padding: int = 30, max_total_lines: int = 260) -> str:\n",
        "    lines = file_text.splitlines()\n",
        "    # convert each hunk into [start, end] range with padding\n",
        "    ranges = []\n",
        "    for start, length in hunks:\n",
        "        s = max(1, start - padding)\n",
        "        e = min(len(lines), start + length + padding)\n",
        "        ranges.append((s,e))\n",
        "\n",
        "    ranges = merge_ranges(ranges)\n",
        "\n",
        "    snippets = []\n",
        "    used = 0\n",
        "    for s,e in ranges:\n",
        "        if used >= max_total_lines:\n",
        "            break\n",
        "        take_s = s\n",
        "        take_e = min(e, s + (max_total_lines - used) - 1)\n",
        "        snippet = \"\\n\".join(lines[take_s-1:take_e])\n",
        "        snippets.append(f\"[lines {take_s}-{take_e}]\\n{snippet}\")\n",
        "        used += (take_e - take_s + 1)\n",
        "\n",
        "    return \"\\n\\n\".join(snippets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the “review bundle” (patch + targeted file context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_pr_header(pr: PRDetails) -> str:\n",
        "    return \"\\n\".join([\n",
        "        f\"PR TITLE: {pr.title}\",\n",
        "        f\"PR AUTHOR: {pr.user.login} ({pr.user.type})\",\n",
        "        f\"CREATED: {pr.created_at.isoformat()} | UPDATED: {pr.updated_at.isoformat()}\",\n",
        "        f\"COMMITS: {pr.commits} | +{pr.additions} -{pr.deletions} | FILES: {pr.changed_files}\",\n",
        "        \"\",\n",
        "        \"PR DESCRIPTION:\",\n",
        "        (pr.body or \"\").strip()[:2000],\n",
        "        \"\",\n",
        "    ])\n",
        "\n",
        "def build_patch_only_bundle(files: List[PRFile], max_patch_chars: int = 6000) -> str:\n",
        "    chunks = []\n",
        "    for f in files:\n",
        "        patch = (f.patch or \"\")\n",
        "        patch = patch[:max_patch_chars] if patch else \"\"\n",
        "        chunks.append(\"\\n\".join([\n",
        "            f\"FILE: {f.filename} | status={f.status} | +{f.additions} -{f.deletions} | changes={f.changes}\",\n",
        "            \"PATCH:\",\n",
        "            patch if patch else \"[No patch provided by GitHub API]\",\n",
        "            \"-\"*60\n",
        "        ]))\n",
        "    return \"\\n\".join(chunks)\n",
        "\n",
        "def build_enriched_bundle(files: List[PRFile], focus_files: List[str], padding: int = 30) -> str:\n",
        "    focus_set = set(focus_files)\n",
        "    chunks = []\n",
        "    for f in files:\n",
        "        if f.filename not in focus_set:\n",
        "            continue\n",
        "\n",
        "        head_text = fetch_text(f.raw_url)\n",
        "        hunks = parse_added_hunks(f.patch)\n",
        "        context = \"\"\n",
        "        if head_text and hunks:\n",
        "            context = extract_windows_from_hunks(head_text, hunks, padding=padding, max_total_lines=260)\n",
        "        elif head_text:\n",
        "            # fallback: top slice\n",
        "            lines = head_text.splitlines()\n",
        "            context = \"\\n\".join(lines[:140]) + (\"\\n...\\n\" if len(lines) > 160 else \"\")\n",
        "\n",
        "        chunks.append(\"\\n\".join([\n",
        "            f\"FILE: {f.filename} | status={f.status} | +{f.additions} -{f.deletions} | changes={f.changes}\",\n",
        "            \"TARGETED HEAD CONTEXT:\",\n",
        "            context if context else \"[Could not fetch/extract head context]\",\n",
        "            \"-\"*60\n",
        "        ]))\n",
        "    return \"\\n\".join(chunks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agent schemas (structured)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "Severity = Literal[\"P0\", \"P1\", \"P2\"]\n",
        "Category = Literal[\"correctness\", \"security\", \"performance\", \"maintainability\"]\n",
        "\n",
        "class Finding(BaseModel):\n",
        "    severity: Severity\n",
        "    category: Category\n",
        "    file: Optional[str] = None\n",
        "    line_range: Optional[str] = None\n",
        "    title: str\n",
        "    description: str\n",
        "    recommendation: str\n",
        "    confidence: float  # 0.0 - 1.0\n",
        "\n",
        "class AgentFindings(BaseModel):\n",
        "    findings: List[Finding]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM agents (Triage + Correctness + Security)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
        "\n",
        "# Triage Agent\n",
        "def triage_agent(pr_header: str, patch_bundle: str, llm) -> TriageResult:\n",
        "    system = \"\"\"You are a senior tech lead doing PR triage.\n",
        "Pick a small set of focus files (max 5) that are most important/risky to review deeply.\n",
        "Only use information in the PR header and patches provided.\n",
        "\"\"\"\n",
        "\n",
        "    user = f\"\"\"Return JSON matching this schema:\n",
        "{TriageResult.model_json_schema()}\n",
        "\n",
        "PR HEADER:\n",
        "{pr_header}\n",
        "\n",
        "PATCHES:\n",
        "{patch_bundle}\n",
        "\"\"\"\n",
        "    structured = llm.with_structured_output(TriageResult)\n",
        "    return structured.invoke([SystemMessage(content=system), HumanMessage(content=user)])\n",
        "\n",
        "# Correctness Agent\n",
        "def correctness_agent(pr_header: str, enriched_bundle: str, llm) -> List[Finding]:\n",
        "    system = \"\"\"You are a correctness-focused code reviewer.\n",
        "Find concrete, high-signal issues grounded in the provided context.\n",
        "Prefer fewer, higher impact findings. If unsure, lower confidence.\n",
        "\"\"\"\n",
        "    user = f\"\"\"Return JSON matching this schema:\n",
        "{AgentFindings.model_json_schema()}\n",
        "\n",
        "PR HEADER:\n",
        "{pr_header}\n",
        "\n",
        "ENRICHED CONTEXT (targeted file windows):\n",
        "{enriched_bundle}\n",
        "\"\"\"\n",
        "    structured = llm.with_structured_output(AgentFindings)\n",
        "    return structured.invoke([SystemMessage(content=system), HumanMessage(content=user)]).findings\n",
        "\n",
        "# Security Agent\n",
        "def security_agent(pr_header: str, enriched_bundle: str, llm) -> List[Finding]:\n",
        "    system = \"\"\"You are a security-focused code reviewer.\n",
        "Look for auth/authz mistakes, secrets leakage, injection risks, unsafe logging, insecure defaults.\n",
        "Only report issues supported by the diff/context. If unsure, ask a question instead of asserting.\n",
        "\"\"\"\n",
        "    user = f\"\"\"Return JSON matching this schema:\n",
        "{AgentFindings.model_json_schema()}\n",
        "\n",
        "PR HEADER:\n",
        "{pr_header}\n",
        "\n",
        "ENRICHED CONTEXT (targeted file windows):\n",
        "{enriched_bundle}\n",
        "\"\"\"\n",
        "    structured = llm.with_structured_output(AgentFindings)\n",
        "    return structured.invoke([SystemMessage(content=system), HumanMessage(content=user)]).findings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lead reviewer: dedupe + rank + render Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dedupe_findings(findings: List[Finding]) -> List[Finding]:\n",
        "    # simple heuristic: same file + same title => same issue\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for f in findings:\n",
        "        key = (f.file or \"\", f.title.strip().lower(), f.category)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        out.append(f)\n",
        "    # sort by severity then confidence desc\n",
        "    sev_order = {\"P0\": 0, \"P1\": 1, \"P2\": 2}\n",
        "    out.sort(key=lambda x: (sev_order.get(x.severity, 9), -x.confidence))\n",
        "    return out\n",
        "\n",
        "def render_findings_md(items: List[Finding]) -> str:\n",
        "    if not items:\n",
        "        return \"_None_\"\n",
        "    lines = []\n",
        "    for f in items:\n",
        "        loc = \"\"\n",
        "        if f.file:\n",
        "            loc = f.file\n",
        "            if f.line_range:\n",
        "                loc += f\":{f.line_range}\"\n",
        "        loc = f\" — `{loc}`\" if loc else \"\"\n",
        "        lines.append(\n",
        "            f\"- **[{f.category.upper()}] {f.title}**{loc}\\n\"\n",
        "            f\"  - {f.description}\\n\"\n",
        "            f\"  - **Recommendation:** {f.recommendation}\\n\"\n",
        "            f\"  - Confidence: `{f.confidence:.2f}`\"\n",
        "        )\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def lead_reviewer(pr: PRDetails, triage: TriageResult, findings: List[Finding]) -> str:\n",
        "    findings = dedupe_findings(findings)\n",
        "    p0 = [f for f in findings if f.severity == \"P0\"]\n",
        "    p1 = [f for f in findings if f.severity == \"P1\"]\n",
        "    p2 = [f for f in findings if f.severity == \"P2\"]\n",
        "\n",
        "    q_md = \"\\n\".join([f\"- {q}\" for q in triage.questions_for_author]) if triage.questions_for_author else \"_None_\"\n",
        "\n",
        "    return f\"\"\"# SentinelReview Report\n",
        "\n",
        "## PR\n",
        "- **Title:** {pr.title}\n",
        "- **Author:** {pr.user.login} ({pr.user.type})\n",
        "- **Files Changed:** {pr.changed_files} | **Commits:** {pr.commits} | **Net:** +{pr.additions} -{pr.deletions}\n",
        "- **Triage Risk:** **{triage.risk.upper()}**\n",
        "\n",
        "## Summary\n",
        "{triage.summary}\n",
        "\n",
        "## Questions for the author\n",
        "{q_md}\n",
        "\n",
        "## Findings\n",
        "\n",
        "### P0 (Must fix)\n",
        "{render_findings_md(p0)}\n",
        "\n",
        "### P1 (Should fix)\n",
        "{render_findings_md(p1)}\n",
        "\n",
        "### P2 (Nice to have)\n",
        "{render_findings_md(p2)}\n",
        "\n",
        "## Focus files reviewed deeply\n",
        "{\", \".join(triage.focus_files) if triage.focus_files else \"_None_\"}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End-to-end run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[61], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m patch_bundle \u001b[38;5;241m=\u001b[39m build_patch_only_bundle(files)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 1) TRIAGE decides focus files\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m triage \u001b[38;5;241m=\u001b[39m \u001b[43mtriage_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpr_header\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_bundle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTriage focus files:\u001b[39m\u001b[38;5;124m\"\u001b[39m, triage\u001b[38;5;241m.\u001b[39mfocus_files)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 2) Build enriched context only for focus files\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[59], line 20\u001b[0m, in \u001b[0;36mtriage_agent\u001b[0;34m(pr_header, patch_bundle, llm)\u001b[0m\n\u001b[1;32m     10\u001b[0m     user \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mReturn JSON matching this schema:\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mTriageResult\u001b[38;5;241m.\u001b[39mmodel_json_schema()\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mpatch_bundle\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     19\u001b[0m     structured \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mwith_structured_output(TriageResult)\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstructured\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/Personal_Projects/SentinelReview/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:3149\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m   3148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3149\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3150\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3151\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config)\n",
            "File \u001b[0;32m~/Desktop/Personal_Projects/SentinelReview/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:5557\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5550\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   5551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5552\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5555\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5556\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5558\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5559\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5560\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/Personal_Projects/SentinelReview/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    392\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AIMessage:\n\u001b[1;32m    393\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    396\u001b[0m         cast(\n\u001b[1;32m    397\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    408\u001b[0m         )\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[1;32m    409\u001b[0m     )\n",
            "File \u001b[0;32m~/Desktop/Personal_Projects/SentinelReview/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1117\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1115\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1116\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/Personal_Projects/SentinelReview/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:927\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    926\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 927\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m         )\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
            "File \u001b[0;32m~/Desktop/Personal_Projects/SentinelReview/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1221\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1221\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1225\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Desktop/Personal_Projects/SentinelReview/.venv/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1380\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_response\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1379\u001b[0m         e\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mhttp_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1380\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_response_headers\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1384\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1385\u001b[0m ):\n\u001b[1;32m   1386\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n",
            "File \u001b[0;32m~/Desktop/Personal_Projects/SentinelReview/.venv/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1348\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m payload\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1348\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1351\u001b[0m     )\n\u001b[1;32m   1352\u001b[0m     response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/Desktop/Personal_Projects/SentinelReview/.venv/lib/python3.10/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/Desktop/Personal_Projects/SentinelReview/.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:184\u001b[0m, in \u001b[0;36mCompletions.parse\u001b[0;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparser\u001b[39m(raw_completion: ChatCompletion) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParsedChatCompletion[ResponseFormatT]:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[1;32m    179\u001b[0m         response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[1;32m    180\u001b[0m         chat_completion\u001b[38;5;241m=\u001b[39mraw_completion,\n\u001b[1;32m    181\u001b[0m         input_tools\u001b[38;5;241m=\u001b[39mchat_completion_tools,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_retention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/Personal_Projects/SentinelReview/.venv/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/Desktop/Personal_Projects/SentinelReview/.venv/lib/python3.10/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "pull_request_url = \"https://github.com/kiranrawat/SentinelReview/pull/1\"\n",
        "\n",
        "\n",
        "#  \"https://github.com/langchain-ai/langgraph/pulls/6641\"\n",
        "# pull_request_url = \"https://github.com/topoteretes/cognee/pull/1851\"\n",
        "\n",
        "\n",
        "fetcher = GithubPRFilesFetcher(pr_url=pull_request_url)\n",
        "pr, files = fetcher.fetch_pr()\n",
        "\n",
        "pr_header = build_pr_header(pr)\n",
        "patch_bundle = build_patch_only_bundle(files)\n",
        "\n",
        "# 1) TRIAGE decides focus files\n",
        "triage = triage_agent(pr_header, patch_bundle, llm)\n",
        "print(\"Triage focus files:\", triage.focus_files)\n",
        "\n",
        "# 2) Build enriched context only for focus files\n",
        "enriched = build_enriched_bundle(files, triage.focus_files, padding=30)\n",
        "\n",
        "# 3) Specialist agents\n",
        "findings = []\n",
        "findings += correctness_agent(pr_header, enriched, llm)\n",
        "findings += security_agent(pr_header, enriched, llm)\n",
        "\n",
        "# 4) Final report\n",
        "report_md = lead_reviewer(pr, triage, findings)\n",
        "print(report_md)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PR TITLE: feat: Simple TUI for cognee-cli\n",
            "PR AUTHOR: rajeevrajeshuni (User)\n",
            "CREATED: 2025-11-30 08:52:14+00:00 | UPDATED: 2025-12-29 16:33:11+00:00\n",
            "COMMITS: 50 | +1710 -25 | FILES: 18\n",
            "\n",
            "PR DESCRIPTION:\n",
            "## Description\n",
            "The TUI is up as a functional starting point, though some flags are missing compared to the CLI.\n",
            "\n",
            "\n",
            "* **Usage:** Added a `tui` the command to the cognee-cli to start the TUI.\n",
            "* **Status:** `add`, `search`, `delete` and `cognify` screens work as expected.\n",
            "* **Blockers:** The `config` command seems incomplete on the existing cli, same is the case with TUI too. \n",
            "* **Plan:** Let's hold off on automated tests until the UI design is final to avoid simulating UI actions on a moving target.\n",
            "\n",
            "Fixes #1762\n",
            "\n",
            "## Type of Change\n",
            "- [x] New feature (non-breaking change that adds functionality)\n",
            "\n",
            "## Pre-submission Checklist\n",
            "- [x] This PR contains minimal changes necessary to address the issue/feature\n",
            "- [x] My code follows the project's coding standards and style guidelines\n",
            "- [x] I have searched existing PRs to ensure this change hasn't been submitted already\n",
            "- [x] I have linked any relevant issues in the description\n",
            "- [x] My commits have clear and descriptive messages\n",
            "\n",
            "## DCO Affirmation\n",
            "I affirm that all code in every commit of this pull request conforms to the terms of the Topoteretes Developer Certificate of Origin.\n",
            "\n",
            "<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n",
            "## Summary by CodeRabbit\n",
            "\n",
            "* **New Features**\n",
            "  * Added an interactive TUI with a menu and screens for Add, Search, Cognify, Delete, and Config, plus a new \"tui\" CLI command to launch it.\n",
            "  * New dataset/user deletion helpers exposed in the public API.\n",
            "\n",
            "* **Bug Fixes / Improvements**\n",
            "  * Batch deletion support and unified SQLite foreign-key handling for bulk deletes.\n",
            "\n",
            "* **Chores**\n",
            "  * Optional textual dependency added; logging setup gained an enable_console_logging option.\n",
            "\n",
            "<sub>✏️ Tip: You can customize this high-level summary in your review settings.</sub>\n",
            "<!-- end of auto-generated comment: release notes by coderabbit.ai -->\n",
            "\n",
            "CHANGED FILES & PATCHES:\n",
            "\n",
            "FILE: cognee/cli/_cognee.py | status=modified | +1 -0 | changes=1\n",
            "PATCH:\n",
            "@@ -92,6 +92,7 @@ def _discover_commands() -> List[Type[SupportsCliCommand]]:\n",
            "         (\"cognee.cli.commands.cognify_command\", \"CognifyCommand\"),\n",
            "         (\"cognee.cli.commands.delete_command\", \"DeleteCommand\"),\n",
            "         (\"cognee.cli.commands.config_command\", \"ConfigCommand\"),\n",
            "+        (\"cognee.cli.commands.tui_command\", \"TuiCommand\"),\n",
            "     ]\n",
            " \n",
            "     for module_path, class_name in command_modules:\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE: cognee/cli/commands/tui_command.py | status=added | +56 -0 | changes=56\n",
            "PATCH:\n",
            "@@ -0,0 +1,56 @@\n",
            "+import argparse\n",
            "+from cognee.cli import SupportsCliCommand\n",
            "+from cognee.cli.config import DEFAULT_DOCS_URL\n",
            "+import cognee.cli.echo as fmt\n",
            "+from cognee.cli.exceptions import CliCommandException\n",
            "+\n",
            "+\n",
            "+class TuiCommand(SupportsCliCommand):\n",
            "+    @property\n",
            "+    def command_string(self) -> str:\n",
            "+        return \"tui\"\n",
            "+\n",
            "+    @property\n",
            "+    def help_string(self) -> str:\n",
            "+        return \"Launch the interactive Textual TUI for cognee commands\"\n",
            "+\n",
            "+    @property\n",
            "+    def docs_url(self) -> str:\n",
            "+        return f\"{DEFAULT_DOCS_URL}/usage/tui\"\n",
            "+\n",
            "+    def configure_parser(self, parser: argparse.ArgumentParser) -> None:\n",
            "+        # No additional arguments for now\n",
            "+        pass\n",
            "+\n",
            "+    def execute(self, args: argparse.Namespace) -> None:\n",
            "+        try:\n",
            "+            from textual.app import App\n",
            "+            from cognee.cli.tui.home_screen import HomeScreen\n",
            "+            from cognee.shared.logging_utils import setup_logging\n",
            "+            class CogneeTUI(App):\n",
            "+                \"\"\"Main TUI application for cognee.\"\"\"\n",
            "+\n",
            "+                CSS = \"\"\"\n",
            "+                Screen {\n",
            "+                    background: $surface;\n",
            "+                }\n",
            "+                \"\"\"\n",
            "+\n",
            "+                def on_mount(self) -> None:\n",
            "+                    \"\"\"Push the home screen on mount.\"\"\"\n",
            "+                    self.push_screen(HomeScreen())\n",
            "+\n",
            "+            setup_logging(enable_console_logging=False)\n",
            "+            app = CogneeTUI()\n",
            "+            app.run()\n",
            "+        except ImportError:\n",
            "+            raise CliCommandException(\n",
            "+                \"Textual is not installed. Install with: pip install textual\",\n",
            "+                docs_url=self.docs_url,\n",
            "+            )\n",
            "+        except Exception as ex:\n",
            "+            raise CliCommandException(\n",
            "+                f\"Failed to launch TUI: {str(ex)}\",\n",
            "+                docs_url=self.docs_url,\n",
            "+                raiseable_exception=ex,\n",
            "+            )\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE: cognee/cli/tui/__init__.py | status=added | +1 -0 | changes=1\n",
            "PATCH:\n",
            "@@ -0,0 +1 @@\n",
            "+\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE: cognee/cli/tui/add_screen.py | status=added | +126 -0 | changes=126\n",
            "PATCH:\n",
            "@@ -0,0 +1,126 @@\n",
            "+import asyncio\n",
            "+from textual.app import ComposeResult\n",
            "+from textual.widgets import Input, Label, Static, TextArea\n",
            "+from textual.containers import Container, Vertical\n",
            "+from textual.binding import Binding\n",
            "+\n",
            "+from cognee.cli.tui.base_screen import BaseTUIScreen\n",
            "+\n",
            "+\n",
            "+class AddTUIScreen(BaseTUIScreen):\n",
            "+    \"\"\"TUI screen for adding data to cognee.\"\"\"\n",
            "+\n",
            "+    BINDINGS = [\n",
            "+        Binding(\"q\", \"quit_app\", \"Quit\"),\n",
            "+        Binding(\"escape\", \"back\", \"Back\"),\n",
            "+        Binding(\"ctrl+s\", \"submit\", \"Submit\"),\n",
            "+        Binding(\"ctrl+v\", \"paste\", \"Paste\", show=False),\n",
            "+    ]\n",
            "+\n",
            "+    CSS = (\n",
            "+        BaseTUIScreen.CSS\n",
            "+        + \"\"\"\n",
            "+    #data-input {\n",
            "+        height: 8;\n",
            "+        min-height: 8;\n",
            "+    }\n",
            "+    \"\"\"\n",
            "+    )\n",
            "+\n",
            "+    def __init__(self):\n",
            "+        super().__init__()\n",
            "+        self.is_processing = False\n",
            "+\n",
            "+    def compose_content(self) -> ComposeResult:\n",
            "+        with Container(classes=\"tui-main-container\"):\n",
            "+            with Container(classes=\"tui-title-wrapper\"):\n",
            "+                yield Static(\"📥 Add Data to Cognee\", classes=\"tui-title-bordered\")\n",
            "+            with Vertical(classes=\"tui-form\"):\n",
            "+                yield Label(\n",
            "+                    \"Data (text, file path (/path/to/doc), URL, or S3 path (s3://bucket)):\",\n",
            "+                    classes=\"tui-label-spaced\",\n",
            "+                )\n",
            "+                yield TextArea(\n",
            "+                    \"\",\n",
            "+                    id=\"data-input\",\n",
            "+                )\n",
            "+\n",
            "+                yield Label(\"Dataset Name:\", classes=\"tui-label-spaced\")\n",
            "+                yield Input(placeholder=\"main_dataset\", value=\"main_dataset\", id=\"dataset-input\")\n",
            "+            yield Static(\"\", classes=\"tui-status\")\n",
            "+\n",
            "+    def compose_footer(self) -> ComposeResult:\n",
            "+        yield Static(\"Ctrl+S: Add  •  Esc: Back  •  q: Quit\", classes=\"tui-footer\")\n",
            "+\n",
            "+    def on_mount(self) -> None:\n",
            "+        \"\"\"Focus the data input on mount.\"\"\"\n",
            "+        data_input = self.query_one(\"#data-input\", TextArea)\n",
            "+        data_input.focus()\n",
            "+\n",
            "+    def action_back(self) -> None:\n",
            "+        \"\"\"Go back to home screen.\"\"\"\n",
            "+        if not self.is_processing:\n",
            "+            self.app.pop_screen()\n",
            "+\n",
            "+    def action_quit_app(self) -> None:\n",
            "+        \"\"\"Quit the entire application.\"\"\"\n",
            "+        self.app.exit()\n",
            "+\n",
            "+    def action_paste(self) -> None:\n",
            "+        \"\"\"Handle paste action - Textual handles this automatically.\"\"\"\n",
            "+        pass\n",
            "+\n",
            "+    def action_submit(self) -> None:\n",
            "+        \"\"\"Submit the form.\"\"\"\n",
            "+        if not self.is_processing:\n",
            "+            self._submit_data()\n",
            "+\n",
            "+    def _submit_data(self) -> None:\n",
            "+        \"\"\"Process and submit the data.\"\"\"\n",
            "+        data_input = self.query_one(\"#data-input\", TextArea)\n",
            "+        dataset_input = self.query_one(\"#dataset-input\", Input)\n",
            "+        status = self.query_one(\".tui-status\", Static)\n",
            "+\n",
            "+        data = data_input.text.strip()\n",
            "+        dataset_name = dataset_input.value.strip() or \"main_dataset\"\n",
            "+\n",
            "+        if not data:\n",
            "+            status.update(\"[red]✗ Please enter data to add[/red]\")\n",
            "+            return\n",
            "+\n",
            "+        self.is_processing = True\n",
            "+        status.update(\"[yellow]⏳ Processing...[/yellow]\")\n",
            "+\n",
            "+        # Disable inputs during processing\n",
            "+        data_input.disabled = True\n",
            "+        dataset_input.disabled = True\n",
            "+\n",
            "+        # Run async add operation\n",
            "+        asyncio.create_task(self._add_data_async(data, dataset_name))\n",
            "+\n",
            "+    async def _add_data_async(self, data: str, dataset_name: str) -> None:\n",
            "+        \"\"\"Async function to add data to cognee.\"\"\"\n",
            "+        status = self.query_one(\".tui-status\", Static)\n",
            "+\n",
            "+        try:\n",
            "+            import cognee\n",
            "+\n",
            "+            await cognee.add(data=data, dataset_name=dataset_name)\n",
            "+\n",
            "+            status.update(f\"[green]✓ Successfully added data to dataset '{dataset_name}'[/green]\")\n",
            "+\n",
            "+            # Clear the data input after successful add\n",
            "+            data_input = self.query_one(\"#data-input\", TextArea)\n",
            "+            data_input.clear()\n",
            "+\n",
            "+        except Exception as e:\n",
            "+            status.update(f\"[red]✗ Failed to add data: {str(e)}[/red]\")\n",
            "+\n",
            "+        finally:\n",
            "+            # Re-enable inputs\n",
            "+            self.is_processing = False\n",
            "+            data_input = self.query_one(\"#data-input\", TextArea)\n",
            "+            dataset_input = self.query_one(\"#dataset-input\", Input)\n",
            "+            data_input.disabled = False\n",
            "+            dataset_input.disabled = False\n",
            "+            data_input.focus()\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE: cognee/cli/tui/base_screen.py | status=added | +45 -0 | changes=45\n",
            "PATCH:\n",
            "@@ -0,0 +1,45 @@\n",
            "+from textual.screen import Screen\n",
            "+from textual.app import ComposeResult\n",
            "+from textual.widgets import Static\n",
            "+\n",
            "+from cognee.version import get_cognee_version\n",
            "+from cognee.cli.tui.common_styles import COMMON_STYLES\n",
            "+\n",
            "+\n",
            "+class BaseTUIScreen(Screen):\n",
            "+    \"\"\"Base screen class with constant header for all TUI screens.\"\"\"\n",
            "+\n",
            "+    # Subclasses should override this CSS and add their own styles\n",
            "+    CSS = (\n",
            "+        COMMON_STYLES\n",
            "+        + \"\"\"\n",
            "+    #header {\n",
            "+        dock: top;\n",
            "+        background: $boost;\n",
            "+        color: $text;\n",
            "+        content-align: center middle;\n",
            "+        border: solid $primary;\n",
            "+        text-style: bold;\n",
            "+        padding: 1;\n",
            "+    }\n",
            "+    \"\"\"\n",
            "+    )\n",
            "+\n",
            "+    def compose_header(self) -> ComposeResult:\n",
            "+        \"\"\"Compose the constant header widget.\"\"\"\n",
            "+        version = get_cognee_version()\n",
            "+        yield Static(f\"🧠 cognee v{version}\", id=\"header\")\n",
            "+\n",
            "+    def compose_content(self) -> ComposeResult:\n",
            "+        \"\"\"Override this method in subclasses to provide screen-specific content.\"\"\"\n",
            "+        yield from ()\n",
            "+\n",
            "+    def compose_footer(self) -> ComposeResult:\n",
            "+        \"\"\"Override this method in subclasses to provide screen-specific footer.\"\"\"\n",
            "+        yield from ()\n",
            "+\n",
            "+    def compose(self) -> ComposeResult:\n",
            "+        \"\"\"Compose the screen with header, content, and footer.\"\"\"\n",
            "+        yield from self.compose_header()\n",
            "+        yield from self.compose_content()\n",
            "+        yield from self.compose_footer()\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE: cognee/cli/tui/cognify_screen.py | status=added | +169 -0 | changes=169\n",
            "PATCH:\n",
            "@@ -0,0 +1,169 @@\n",
            "+import asyncio\n",
            "+from textual.app import ComposeResult\n",
            "+from textual.widgets import Input, Label, Static, Checkbox, RadioSet, RadioButton\n",
            "+from textual.containers import Container, Vertical\n",
            "+from textual.binding import Binding\n",
            "+from cognee.cli.tui.base_screen import BaseTUIScreen\n",
            "+from cognee.cli.config import CHUNKER_CHOICES\n",
            "+\n",
            "+\n",
            "+class CognifyTUIScreen(BaseTUIScreen):\n",
            "+    \"\"\"TUI screen for cognifying data in cognee.\"\"\"\n",
            "+\n",
            "+    BINDINGS = [\n",
            "+        Binding(\"q\", \"quit_app\", \"Quit\"),\n",
            "+        Binding(\"escape\", \"back\", \"Back\"),\n",
            "+        Binding(\"ctrl+s\", \"submit\", \"Submit\"),\n",
            "+    ]\n",
            "+\n",
            "+    CSS = (\n",
            "+        BaseTUIScreen.CSS\n",
            "+        + \"\"\"\n",
            "+    Checkbox {\n",
            "+        margin-top: 1;\n",
            "+        margin-bottom: 1;\n",
            "+    }\n",
            "+\n",
            "+    RadioSet {\n",
            "+        margin-top: 0;\n",
            "+        margin-bottom: 1;\n",
            "+        height: auto;\n",
            "+    }\n",
            "+\n",
            "+    RadioButton {\n",
            "+        height: 1;\n",
            "+    }\n",
            "+    \"\"\"\n",
            "+    )\n",
            "+\n",
            "+    def __init__(self):\n",
            "+        super().__init__()\n",
            "+        self.is_processing = False\n",
            "+\n",
            "+    def compose_content(self) -> ComposeResult:\n",
            "+        with Container(classes=\"tui-main-container\"):\n",
            "+            with Container(classes=\"tui-title-wrapper\"):\n",
            "+                yield Static(\"⚡ Cognify Data\", classes=\"tui-title-bordered\")\n",
            "+            with Vertical(classes=\"tui-form\"):\n",
            "+                yield Label(\n",
            "+                    \"Dataset Name:\", classes=\"tui-label-spaced\"\n",
            "+                )\n",
            "+                yield Input(\n",
            "+                    placeholder=\"Enter the dataset name here.\", value=\"\", id=\"dataset-input\"\n",
            "+                )\n",
            "+\n",
            "+                yield Label(\"Chunker Type:\", classes=\"tui-label-spaced\")\n",
            "+                with RadioSet(id=\"chunker-radio\"):\n",
            "+                    for chunker in CHUNKER_CHOICES:\n",
            "+                        yield RadioButton(chunker, value=(chunker == \"TextChunker\"))\n",
            "+\n",
            "+                yield Checkbox(\"Run in background\", id=\"background-checkbox\")\n",
            "+            yield Static(\"\", classes=\"tui-status\")\n",
            "+\n",
            "+    def compose_footer(self) -> ComposeResult:\n",
            "+        yield Static(\"Ctrl+S: Start  •  Esc: Back  •  q: Quit\", classes=\"tui-footer\")\n",
            "+\n",
            "+    def on_mount(self) -> None:\n",
            "+        \"\"\"Focus the dataset input on mount.\"\"\"\n",
            "+        dataset_input = self.query_one(\"#dataset-input\", Input)\n",
            "+        dataset_input.focus()\n",
            "+\n",
            "+    def action_back(self) -> None:\n",
            "+        \"\"\"Go back to home screen.\"\"\"\n",
            "+        if not self.is_processing:\n",
            "+            self.app.pop_screen()\n",
            "+\n",
            "+    def action_quit_app(self) -> None:\n",
            "+        \"\"\"Quit the entire application.\"\"\"\n",
            "+        self.app.exit()\n",
            "+\n",
            "+    def action_submit(self) -> None:\n",
            "+        \"\"\"Submit the form.\"\"\"\n",
            "+        if not self.is_processing:\n",
            "+            self._submit_cognify()\n",
            "+\n",
            "+    def _submit_cognify(self) -> None:\n",
            "+        \"\"\"Process and submit the cognify request.\"\"\"\n",
            "+        dataset_input = self.query_one(\"#dataset-input\", Input)\n",
            "+        chunker_radio = self.query_one(\"#chunker-radio\", RadioSet)\n",
            "+        background_checkbox = self.query_one(\"#background-checkbox\", Checkbox)\n",
            "+        status = self.query_one(\".tui-status\", Static)\n",
            "+\n",
            "+        dataset_name = dataset_input.value.strip() or None\n",
            "+        chunker_type = (\n",
            "+            str(chunker_radio.pressed_button.label)\n",
            "+            if chunker_radio.pressed_button\n",
            "+            else \"TextChunker\"\n",
            "+        )\n",
            "+        run_background = background_checkbox.value\n",
            "+\n",
            "+        self.is_processing = True\n",
            "+        status.update(\"[yellow]⏳ Starting cognification...[/yellow]\")\n",
            "+\n",
            "+        # Disable inputs during processing\n",
            "+        dataset_input.disabled = True\n",
            "+        chunker_radio.disabled = True\n",
            "+        background_checkbox.disabled = True\n",
            "+\n",
            "+        # Run async cognify operation\n",
            "+        asyncio.create_task(self._cognify_async(dataset_name, chunker_type, run_background))\n",
            "+\n",
            "+    async def _cognify_async(\n",
            "+        self, dataset_name: str | None, chunker_type: str, run_background: bool\n",
            "+    ) -> None:\n",
            "+        \"\"\"Async function to cognify data.\"\"\"\n",
            "+        status = self.query_one(\".tui-status\", Static)\n",
            "+        from cognee.modules.chunking.TextChunker import TextChunker\n",
            "+\n",
            "+        try:\n",
            "+            # Get chunker class\n",
            "+            chunker_class = TextChunker\n",
            "+            if chunker_type == \"LangchainChunker\":\n",
            "+                try:\n",
            "+                    from cognee.modules.chunking.LangchainChunker import LangchainChunker\n",
            "+                except ImportError:\n",
            "+                    LangchainChunker = None\n",
            "+                if LangchainChunker is not None:\n",
            "+                    chunker_class = LangchainChunker\n",
            "+                else:\n",
            "+                    status.update(\n",
            "+                        \"[yellow]⚠ LangchainChunker not available, using TextChunker[/yellow]\"\n",
            "+                    )\n",
            "+            elif chunker_type == \"CsvChunker\":\n",
            "+                try:\n",
            "+                    from cognee.modules.chunking.CsvChunker import CsvChunker\n",
            "+                except ImportError:\n",
            "+                    CsvChunker = None\n",
            "+                if CsvChunker is not None:\n",
            "+                    chunker_class = CsvChunker\n",
            "+                else:\n",
            "+                    status.update(\"[yellow]⚠ CsvChunker not available, using TextChunker[/yellow]\")\n",
            "+\n",
            "+            # Prepare datasets parameter\n",
            "+            datasets = [dataset_name] if dataset_name else None\n",
            "+            import cognee\n",
            "+\n",
            "+            await cognee.cognify(\n",
            "+                datasets=datasets,\n",
            "+                chunker=chunker_class,\n",
            "+                run_in_background=run_background,\n",
            "+            )\n",
            "+\n",
            "+            if run_background:\n",
            "+                status.update(\"[green]✓ Cognification started in background![/green]\")\n",
            "+            else:\n",
            "+                status.update(\"[green]✓ Cognification completed successfully![/green]\")\n",
            "+\n",
            "+        except Exception as e:\n",
            "+            status.update(f\"[red]✗ Failed to cognify: {str(e)}[/red]\")\n",
            "+\n",
            "+        finally:\n",
            "+            # Re-enable inputs\n",
            "+            self.is_processing = False\n",
            "+            dataset_input = self.query_one(\"#dataset-input\", Input)\n",
            "+            chunker_radio = self.query_one(\"#chunker-radio\", RadioSet)\n",
            "+            background_checkbox = sel\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE: cognee/cli/tui/common_styles.py | status=added | +135 -0 | changes=135\n",
            "PATCH:\n",
            "@@ -0,0 +1,135 @@\n",
            "+\"\"\"Common CSS styles for TUI screens to reduce repetition.\"\"\"\n",
            "+\n",
            "+COMMON_STYLES = \"\"\"\n",
            "+/* Common screen background */\n",
            "+Screen {\n",
            "+    background: $surface;\n",
            "+}\n",
            "+\n",
            "+/* Common container styles */\n",
            "+.tui-container {\n",
            "+    height: 100%;\n",
            "+    padding: 1;\n",
            "+}\n",
            "+\n",
            "+.tui-bordered-wrapper {\n",
            "+    border: solid $primary;\n",
            "+}\n",
            "+\n",
            "+.tui-content-container {\n",
            "+    height: auto;\n",
            "+    padding: 1;\n",
            "+    content-align: center middle;\n",
            "+}\n",
            "+\n",
            "+/* Main container wrapper - used across all screens */\n",
            "+.tui-main-container {\n",
            "+    height: 100%;\n",
            "+    background: $surface;\n",
            "+}\n",
            "+\n",
            "+/* Title wrapper - centers title elements */\n",
            "+.tui-title-wrapper {\n",
            "+    width: 100%;\n",
            "+    height: auto;\n",
            "+    align: center middle;\n",
            "+    content-align: center middle;\n",
            "+}\n",
            "+\n",
            "+/* Styled title with border */\n",
            "+.tui-title-bordered {\n",
            "+    text-align: center;\n",
            "+    width: auto;\n",
            "+    color: $accent;\n",
            "+    text-style: bold;\n",
            "+    padding: 0 10;\n",
            "+    border: solid $accent;\n",
            "+}\n",
            "+\n",
            "+.tui-form {\n",
            "+    width: 100%;\n",
            "+    height: auto;\n",
            "+    border: solid $primary;\n",
            "+    padding: 2;\n",
            "+    background: $surface;\n",
            "+}\n",
            "+\n",
            "+/* Common title styles */\n",
            "+.tui-title {\n",
            "+    text-align: center;\n",
            "+    text-style: bold;\n",
            "+    color: $accent;\n",
            "+    margin-bottom: 2;\n",
            "+    width: 100%;\n",
            "+}\n",
            "+\n",
            "+/* Common label styles */\n",
            "+.tui-label {\n",
            "+    color: $text-muted;\n",
            "+    margin-bottom: 1;\n",
            "+}\n",
            "+\n",
            "+.tui-label-spaced {\n",
            "+    color: $text-muted;\n",
            "+    margin-top: 1;\n",
            "+    margin-bottom: 1;\n",
            "+}\n",
            "+\n",
            "+/* Common input styles */\n",
            "+Input {\n",
            "+    width: 100%;\n",
            "+    margin-bottom: 1;\n",
            "+}\n",
            "+\n",
            "+/* Common button styles */\n",
            "+Button {\n",
            "+    margin: 0 1;\n",
            "+}\n",
            "+\n",
            "+/* Common status message styles */\n",
            "+.tui-status {\n",
            "+    text-align: center;\n",
            "+    margin-top: 2;\n",
            "+    height: auto;\n",
            "+}\n",
            "+\n",
            "+/* Common footer styles */\n",
            "+.tui-footer {\n",
            "+    dock: bottom;\n",
            "+    padding: 1 0;\n",
            "+    background: $boost;\n",
            "+    color: $text-muted;\n",
            "+    content-align: center middle;\n",
            "+    border: solid $primary;\n",
            "+}\n",
            "+\n",
            "+/* Common dialog/modal styles */\n",
            "+.tui-dialog {\n",
            "+    border: thick $warning;\n",
            "+    background: $surface;\n",
            "+    padding: 2;\n",
            "+}\n",
            "+\n",
            "+.tui-dialog-title {\n",
            "+    text-align: center;\n",
            "+    text-style: bold;\n",
            "+    color: $warning;\n",
            "+    margin-bottom: 1;\n",
            "+}\n",
            "+\n",
            "+.tui-dialog-message {\n",
            "+    text-align: center;\n",
            "+    margin-bottom: 1;\n",
            "+}\n",
            "+\n",
            "+.tui-dialog-buttons {\n",
            "+    align: center middle;\n",
            "+    height: 3;\n",
            "+}\n",
            "+\n",
            "+/* Common input group styles */\n",
            "+.tui-input-group {\n",
            "+    height: auto;\n",
            "+    margin-bottom: 2;\n",
            "+}\n",
            "+\"\"\"\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE: cognee/cli/tui/config_screen.py | status=added | +404 -0 | changes=404\n",
            "PATCH:\n",
            "@@ -0,0 +1,404 @@\n",
            "+import argparse\n",
            "+import json\n",
            "+from typing import Any, Optional\n",
            "+\n",
            "+from cognee.cli.reference import SupportsCliCommand\n",
            "+from cognee.cli import DEFAULT_DOCS_URL\n",
            "+from cognee.cli.exceptions import CliCommandException\n",
            "+\n",
            "+try:\n",
            "+    from textual.app import App, ComposeResult\n",
            "+    from textual.screen import Screen\n",
            "+    from textual.widgets import DataTable, Input, Label, Button, Static\n",
            "+    from textual.containers import Container, Horizontal\n",
            "+    from textual.binding import Binding\n",
            "+    from cognee.cli.tui.base_screen import BaseTUIScreen\n",
            "+except ImportError:\n",
            "+    # Handle case where textual is not installed to prevent import errors at module level\n",
            "+    BaseTUIScreen = object\n",
            "+\n",
            "+\n",
            "+class ConfirmModal(Screen):\n",
            "+    \"\"\"Modal screen for confirming reset action.\"\"\"\n",
            "+\n",
            "+    BINDINGS = [\n",
            "+        Binding(\"escape\", \"cancel\", \"Cancel\"),\n",
            "+    ]\n",
            "+\n",
            "+    CSS = \"\"\"\n",
            "+    ConfirmModal {\n",
            "+        align: center middle;\n",
            "+    }\n",
            "+\n",
            "+    #confirm-dialog {\n",
            "+        width: 60;\n",
            "+        height: auto;\n",
            "+        border: thick $warning;\n",
            "+        background: $surface;\n",
            "+        padding: 1 2;\n",
            "+    }\n",
            "+\n",
            "+    #confirm-title {\n",
            "+        text-align: center;\n",
            "+        text-style: bold;\n",
            "+        margin-bottom: 1;\n",
            "+    }\n",
            "+\n",
            "+    #confirm-message {\n",
            "+        text-align: center;\n",
            "+        margin-bottom: 2;\n",
            "+    }\n",
            "+\n",
            "+    .tui-dialog-buttons {\n",
            "+        align: center middle;\n",
            "+        height: auto;\n",
            "+    }\n",
            "+\n",
            "+    Button {\n",
            "+        margin: 0 1;\n",
            "+    }\n",
            "+    \"\"\"\n",
            "+\n",
            "+    def __init__(self, key: str, default_value: str):\n",
            "+        super().__init__()\n",
            "+        self.key = key\n",
            "+        self.default_value = default_value\n",
            "+\n",
            "+    def compose(self) -> ComposeResult:\n",
            "+        with Container(id=\"confirm-dialog\"):\n",
            "+            yield Label(\"⚠ Reset Configuration\", id=\"confirm-title\")\n",
            "+            yield Label(f\"Are you sure you want to reset '{self.key}'?\", id=\"confirm-message\")\n",
            "+            yield Label(f\"It will revert to: {self.default_value}\", classes=\"dim-text\")\n",
            "+\n",
            "+            with Horizontal(classes=\"tui-dialog-buttons\"):\n",
            "+                yield Button(\"Reset\", variant=\"error\", id=\"confirm-btn\")\n",
            "+                yield Button(\"Cancel\", variant=\"default\", id=\"cancel-btn\")\n",
            "+\n",
            "+    def on_button_pressed(self, event: Button.Pressed) -> None:\n",
            "+        if event.button.id == \"confirm-btn\":\n",
            "+            self.dismiss(True)\n",
            "+        else:\n",
            "+            self.dismiss(False)\n",
            "+\n",
            "+    def action_cancel(self) -> None:\n",
            "+        self.dismiss(False)\n",
            "+\n",
            "+\n",
            "+class ConfigTUIScreen(BaseTUIScreen):\n",
            "+    \"\"\"Main config TUI screen with inline editing and live data fetching.\"\"\"\n",
            "+\n",
            "+    BINDINGS = [\n",
            "+        Binding(\"q\", \"quit_app\", \"Quit\"),\n",
            "+        Binding(\"escape\", \"cancel_or_back\", \"Back/Cancel\"),\n",
            "+        Binding(\"e\", \"edit\", \"Edit\"),\n",
            "+        Binding(\"enter\", \"confirm_edit\", \"Confirm\", show=False),\n",
            "+        Binding(\"r\", \"reset\", \"Reset\"),\n",
            "+        Binding(\"up\", \"cursor_up\", \"Up\", show=False),\n",
            "+        Binding(\"down\", \"cursor_down\", \"Down\", show=False),\n",
            "+    ]\n",
            "+\n",
            "+    CSS = (\n",
            "+        BaseTUIScreen.CSS\n",
            "+        + \"\"\"\n",
            "+    DataTable {\n",
            "+        height: 1fr;\n",
            "+        text-align: center;\n",
            "+    }\n",
            "+\n",
            "+    #inline-edit-container {\n",
            "+        display: none;\n",
            "+        height: auto;\n",
            "+        padding: 0 1;\n",
            "+        margin-top: 1;\n",
            "+    }\n",
            "+\n",
            "+    #inline-edit-container.visible {\n",
            "+        display: block;\n",
            "+    }\n",
            "+\n",
            "+    #edit-label {\n",
            "+        color: $text-muted;\n",
            "+        margin-bottom: 1;\n",
            "+    }\n",
            "+\n",
            "+    #inline-input {\n",
            "+        width: 100%;\n",
            "+    }\n",
            "+\n",
            "+    .dim-text {\n",
            "+        color: $text-muted;\n",
            "+        text-align: center;\n",
            "+        margin-bottom: 1;\n",
            "+    }\n",
            "+    \"\"\"\n",
            "+    )\n",
            "+\n",
            "+    # Config key mappings: Key -> (Reset Method Name, Default Value)\n",
            "+    CONFIG_MAP = {\n",
            "+        \"llm_provider\": (\"set_llm_provider\", \"openai\"),\n",
            "+        \"llm_model\": (\"set_llm_model\", \"gpt-5-mini\"),\n",
            "+        \"llm_api_key\": (\"set_llm_api_key\", \"\"),\n",
            "+        \"llm_endpoint\": (\"set_llm_endpoint\", \"\"),\n",
            "+        \"graph_database_provider\": (\"set_graph_database_provider\", \"kuzu\"),\n",
            "+        \"vector_db_provider\": (\"set_vector_db_provider\", \"lancedb\"),\n",
            "+        \"vector_db_url\": (\"set_vector_db_url\", \"\"),\n",
            "+        \"vector_db_key\": (\"set_vector_db_key\", \"\"),\n",
            "+        \"chunk_size\": (\"set_chunk_size\", 1500),\n",
            "+        \"chunk_overlap\": (\"set_chunk_overlap\", 10),\n",
            "+    }\n",
            "+\n",
            "+    def __init__(self):\n",
            "+        super().__init__()\n",
            "+        self.editing_key = None  # Track which key is being edited\n",
            "+\n",
            "+    def compose_content(self) -> ComposeResult:\n",
            "+        with Container(classes=\"tui-main-container\"):\n",
            "+            with Container(classes=\"tui-title-wrapper\"):\n",
            "+                yield Static(\"⚙️  Configuration Manager\", classes=\"tui-title-bordered\")\n",
            "+\n",
            "+            with Container(classes=\"tui-bordered-wrapper\"):\n",
            "+                table = DataTable(id=\"config-table\")\n",
            "+                table.cursor_type = \"row\"\n",
            "+                table.zebra_stripes = True\n",
            "+                yield table\n",
            "+\n",
            "+                with Container(id=\"inline-edit-container\"):\n",
            "+                    yield Label(\"\", id=\"edit-label\")\n",
            "+                    yield Input(placeholder=\"Enter new value\", id=\"inline-input\")\n",
            "+\n",
            "+    def compose_footer(self) -> ComposeResult:\n",
            "+        yield Static(\n",
            "+            \"↑↓: Navigate  •  e: Edit  •  Enter: Save  •  r: Reset  •  Esc: Back  •  q: Quit\",\n",
            "+            classes=\"tui-footer\",\n",
            "+        )\n",
            "+\n",
            "+    def on_mount(self) -> None:\n",
            "+        \"\"\"Initialize the table with columns and current data.\"\"\"\n",
            "+        table = self.query_one(DataTable)\n",
            "+        table.add_columns(\"Configuration Key\", \"Current Value\")\n",
            "+\n",
            "+        self._load_table_data()\n",
            "+        table.focus()\n",
            "+\n",
            "+    def _load_table_data(self) -> None:\n",
            "+        \"\"\"Fetch real config values and populate the table.\"\"\"\n",
            "+        table = self.query_one(DataTable)\n",
            "+        table.clear()\n",
            "+\n",
            "+        try:\n",
            "+            import cognee\n",
            "+\n",
            "+            # Check if get method exists, otherwise warn\n",
            "+            has_get = hasattr(cognee.config, \"get\")\n",
            "+        except ImportError:\n",
            "+            has_get = False\n",
            "+            self.notify(\"Could not import cognee config\", severity=\"error\")\n",
            "+\n",
            "+        for key, (_, default_val) in self.CONFI\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE: cognee/cli/tui/delete_screen.py | status=added | +247 -0 | changes=247\n",
            "PATCH:\n",
            "@@ -0,0 +1,247 @@\n",
            "+import asyncio\n",
            "+from uuid import UUID\n",
            "+from textual.app import ComposeResult\n",
            "+from textual.widgets import Input, Button, Static, Label\n",
            "+from textual.containers import Container, Vertical, Horizontal\n",
            "+from textual.binding import Binding\n",
            "+from cognee.cli.tui.base_screen import BaseTUIScreen\n",
            "+from cognee.modules.data.methods.delete_dataset_by_name import delete_dataset_by_name\n",
            "+from cognee.modules.data.methods.delete_data_by_user import delete_data_by_user\n",
            "+from cognee.modules.users.methods import get_default_user\n",
            "+\n",
            "+\n",
            "+class DeleteTUIScreen(BaseTUIScreen):\n",
            "+    \"\"\"Simple delete screen with input fields for dataset name, user ID, or delete all.\"\"\"\n",
            "+\n",
            "+    BINDINGS = [\n",
            "+        Binding(\"q\", \"quit_app\", \"Quit\"),\n",
            "+        Binding(\"escape\", \"back\", \"Back\"),\n",
            "+        Binding(\"ctrl+s\", \"delete\", \"Delete\"),\n",
            "+        Binding(\"ctrl+a\", \"delete_all\", \"Delete All\", priority=True),\n",
            "+    ]\n",
            "+\n",
            "+    CSS = (\n",
            "+        BaseTUIScreen.CSS\n",
            "+        + \"\"\"\n",
            "+    #button-group {\n",
            "+        height: auto;\n",
            "+        align: center middle;\n",
            "+        margin-top: 2;\n",
            "+    }\n",
            "+    \"\"\"\n",
            "+    )\n",
            "+\n",
            "+    def __init__(self):\n",
            "+        super().__init__()\n",
            "+        self.is_processing = False\n",
            "+\n",
            "+    def compose_content(self) -> ComposeResult:\n",
            "+        with Container(classes=\"tui-main-container\"):\n",
            "+            with Container(classes=\"tui-title-wrapper\"):\n",
            "+                yield Static(\"🗑  Delete Data\", classes=\"tui-title-bordered\")\n",
            "+            with Vertical(id=\"delete-form\", classes=\"tui-form\"):\n",
            "+                with Vertical(classes=\"tui-input-group\"):\n",
            "+                    yield Label(\"Dataset Name (optional):\", classes=\"tui-label\")\n",
            "+                    yield Input(\n",
            "+                        placeholder=\"Enter dataset name to delete specific dataset\",\n",
            "+                        id=\"dataset-input\",\n",
            "+                    )\n",
            "+\n",
            "+                with Vertical(classes=\"tui-input-group\"):\n",
            "+                    yield Label(\"User ID (optional):\", classes=\"tui-label\")\n",
            "+                    yield Input(\n",
            "+                        placeholder=\"Enter user ID to delete user's data or leave empty for default user.\",\n",
            "+                        id=\"user-input\",\n",
            "+                    )\n",
            "+\n",
            "+                with Horizontal(id=\"button-group\"):\n",
            "+                    yield Button(\"Delete\", variant=\"error\", id=\"delete-btn\")\n",
            "+                    yield Button(\"Delete All\", variant=\"error\", id=\"delete-all-btn\")\n",
            "+\n",
            "+                yield Static(\"\", classes=\"tui-status\")\n",
            "+\n",
            "+    def compose_footer(self) -> ComposeResult:\n",
            "+        yield Static(\n",
            "+            \"Ctrl+s: Delete  •  Ctrl+a: Delete All  •  Esc: Back  •  q: Quit\", classes=\"tui-footer\"\n",
            "+        )\n",
            "+\n",
            "+    def on_mount(self) -> None:\n",
            "+        \"\"\"Focus the dataset input on mount.\"\"\"\n",
            "+        dataset_input = self.query_one(\"#dataset-input\", Input)\n",
            "+        dataset_input.focus()\n",
            "+\n",
            "+    def action_back(self) -> None:\n",
            "+        \"\"\"Go back to home screen.\"\"\"\n",
            "+        if not self.is_processing:\n",
            "+            self.app.pop_screen()\n",
            "+\n",
            "+    def action_quit_app(self) -> None:\n",
            "+        \"\"\"Quit the entire application.\"\"\"\n",
            "+        self.app.exit()\n",
            "+\n",
            "+    def action_delete(self) -> None:\n",
            "+        \"\"\"Delete the dataset.\"\"\"\n",
            "+        if not self.is_processing:\n",
            "+            self._handle_delete()\n",
            "+\n",
            "+    def action_delete_all(self) -> None:\n",
            "+        \"\"\"Delete all data.\"\"\"\n",
            "+        if not self.is_processing:\n",
            "+            self._handle_delete_all()\n",
            "+\n",
            "+    def on_button_pressed(self, event: Button.Pressed) -> None:\n",
            "+        \"\"\"Handle button presses.\"\"\"\n",
            "+        if self.is_processing:\n",
            "+            return\n",
            "+        if event.button.id == \"delete-btn\":\n",
            "+            self._handle_delete()\n",
            "+        elif event.button.id == \"delete-all-btn\":\n",
            "+            self._handle_delete_all()\n",
            "+        elif event.button.id == \"cancel-btn\":\n",
            "+            self.app.pop_screen()\n",
            "+\n",
            "+    def _handle_delete(self) -> None:\n",
            "+        status = self.query_one(\".tui-status\", Static)\n",
            "+        status.update(\"🔍 Starting the deletion process...\")\n",
            "+        \"\"\"Handle delete operation for dataset or user.\"\"\"\n",
            "+        if self.is_processing:\n",
            "+            return\n",
            "+\n",
            "+        dataset_input = self.query_one(\"#dataset-input\", Input)\n",
            "+        user_input = self.query_one(\"#user-input\", Input)\n",
            "+\n",
            "+        dataset_name = dataset_input.value.strip() or None\n",
            "+        user_id = user_input.value.strip() or None\n",
            "+\n",
            "+        if not dataset_name and not user_id:\n",
            "+            status.update(\"⚠️  Please enter a dataset name or user ID\")\n",
            "+            return\n",
            "+\n",
            "+        self.is_processing = True\n",
            "+        status.update(\"🔍 Checking data to delete...\")\n",
            "+        # Run async delete operation\n",
            "+        asyncio.create_task(self._delete_async(dataset_name, user_id))\n",
            "+\n",
            "+    async def _delete_async(self, dataset_name: str | None, user_id: str | None) -> None:\n",
            "+        \"\"\"Async function to delete data.\"\"\"\n",
            "+        status = self.query_one(\".tui-status\", Static)\n",
            "+        try:\n",
            "+            if user_id is None:\n",
            "+                user = await get_default_user()\n",
            "+                resolved_user_id = user.id\n",
            "+            else:\n",
            "+                resolved_user_id = UUID(user_id)\n",
            "+\n",
            "+            if dataset_name:\n",
            "+                await delete_dataset_by_name(dataset_name, resolved_user_id)\n",
            "+                status.update(f\"✓ Successfully deleted dataset '{dataset_name}'.\")\n",
            "+            else:\n",
            "+                await delete_data_by_user(resolved_user_id)\n",
            "+                status.update(f\"✓ Successfully deleted all data for user {resolved_user_id}.\")\n",
            "+        except Exception as e:\n",
            "+            status.update(f\"✗ Error: {str(e)}\")\n",
            "+        finally:\n",
            "+            self.is_processing = False\n",
            "+            self.clear_input()\n",
            "+\n",
            "+    def _handle_delete_all(self) -> None:\n",
            "+        \"\"\"Handle delete all operation with confirmation.\"\"\"\n",
            "+        if self.is_processing:\n",
            "+            return\n",
            "+        user_input = self.query_one(\"#user-input\", Input)\n",
            "+        user_id = user_input.value.strip() or None\n",
            "+\n",
            "+        def handle_confirm(confirmed: bool) -> None:\n",
            "+            if confirmed:\n",
            "+                asyncio.create_task(self._perform_delete_all(user_id))\n",
            "+\n",
            "+        se\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE: cognee/cli/tui/home_screen.py | status=added | +188 -0 | changes=188\n",
            "PATCH:\n",
            "@@ -0,0 +1,188 @@\n",
            "+from textual.app import ComposeResult\n",
            "+from textual.widgets import ListView, ListItem, Static\n",
            "+from textual.containers import Container, Horizontal\n",
            "+from textual.binding import Binding\n",
            "+\n",
            "+from cognee.cli.tui.base_screen import BaseTUIScreen\n",
            "+from cognee.cli.tui.config_screen import ConfigTUIScreen\n",
            "+from cognee.cli.tui.add_screen import AddTUIScreen\n",
            "+from cognee.cli.tui.cognify_screen import CognifyTUIScreen\n",
            "+from cognee.cli.tui.search_screen import SearchTUIScreen\n",
            "+from cognee.cli.tui.delete_screen import DeleteTUIScreen\n",
            "+\n",
            "+\n",
            "+def make_item(icon: str, command: str, description: str) -> ListItem:\n",
            "+    \"\"\"Compose a ListItem that contains a Horizontal container with 3 children.\"\"\"\n",
            "+    return ListItem(\n",
            "+        Horizontal(\n",
            "+            Static(icon, classes=\"cmd-icon\"),\n",
            "+            Static(command, classes=\"cmd-name\"),\n",
            "+            Static(description, classes=\"cmd-desc\"),\n",
            "+            classes=\"cmd-row\",\n",
            "+        )\n",
            "+    )\n",
            "+\n",
            "+\n",
            "+class HomeScreen(BaseTUIScreen):\n",
            "+    \"\"\"Home screen with command selection menu.\"\"\"\n",
            "+\n",
            "+    BINDINGS = [\n",
            "+        Binding(\"q\", \"quit_app\", \"Quit\"),\n",
            "+        Binding(\"escape\", \"quit_app\", \"Quit\"),\n",
            "+        Binding(\"enter\", \"select\", \"Select\"),\n",
            "+        Binding(\"up\", \"nav_up\", \"Up\", priority=True),\n",
            "+        Binding(\"down\", \"nav_down\", \"Down\", priority=True),\n",
            "+    ]\n",
            "+\n",
            "+    CSS = (\n",
            "+        BaseTUIScreen.CSS\n",
            "+        + \"\"\"\n",
            "+    ListView > ListItem {\n",
            "+        width: 100%;\n",
            "+        padding: 0;\n",
            "+        margin: 0;\n",
            "+    }\n",
            "+    \n",
            "+    .menu-list > ListItem {\n",
            "+        width: 100%;\n",
            "+        padding: 0;\n",
            "+        margin: 0;\n",
            "+    }\n",
            "+\n",
            "+    .menu-list {\n",
            "+        height: auto;\n",
            "+        background: $surface;\n",
            "+        border: none;\n",
            "+        padding: 0 0;\n",
            "+    }\n",
            "+\n",
            "+    ListView {\n",
            "+        height: auto;\n",
            "+        background: $surface;\n",
            "+        border: none;\n",
            "+        padding: 0 0;\n",
            "+    }\n",
            "+\n",
            "+    ListItem {\n",
            "+        background: $surface;\n",
            "+        color: $text;\n",
            "+        width: 100%;\n",
            "+        height: 3;\n",
            "+    }\n",
            "+    \n",
            "+    ListItem:focus {\n",
            "+        outline: none;\n",
            "+    }\n",
            "+\n",
            "+    ListItem.highlighted {\n",
            "+        background: $primary-darken-3;\n",
            "+        color: $text;\n",
            "+    }\n",
            "+    ListItem.highlighted .cmd-name {\n",
            "+        text-style: bold;\n",
            "+        color: $accent;\n",
            "+    }\n",
            "+\n",
            "+    .cmd-row {\n",
            "+        width: 100%;\n",
            "+        height: auto;\n",
            "+        align-horizontal: left;\n",
            "+        align-vertical: middle;\n",
            "+        height: 1fr;\n",
            "+    }\n",
            "+\n",
            "+    .cmd-icon {\n",
            "+        width: 4;\n",
            "+        text-align: center;\n",
            "+        color: $text-muted;\n",
            "+    }\n",
            "+\n",
            "+    .cmd-name {\n",
            "+        width: 14;\n",
            "+        padding-left: 1;\n",
            "+        text-style: bold;\n",
            "+    }\n",
            "+\n",
            "+    .cmd-desc {\n",
            "+        width: 1fr;\n",
            "+        overflow: auto;\n",
            "+        padding-left: 1;\n",
            "+        color: $text-muted;\n",
            "+    }\n",
            "+    \"\"\"\n",
            "+    )\n",
            "+\n",
            "+    def __init__(self):\n",
            "+        super().__init__()\n",
            "+        self.lv = None\n",
            "+        self.current_index = 0\n",
            "+\n",
            "+    def compose_content(self) -> ComposeResult:\n",
            "+        with Container(classes=\"tui-main-container\"):\n",
            "+            with Container(classes=\"tui-title-wrapper\"):\n",
            "+                yield Static(\"Select Command\", classes=\"tui-title-bordered\")\n",
            "+            with Container(classes=\"tui-bordered-wrapper\"):\n",
            "+                yield ListView(\n",
            "+                    make_item(\"📥\", \"add\", \"Add data to cognee\"),\n",
            "+                    make_item(\"🔍\", \"search\", \"Search data in cognee\"),\n",
            "+                    make_item(\"⚡\", \"cognify\", \"Process data in cognee\"),\n",
            "+                    make_item(\"🗑️\", \"delete\", \"Delete data from cognee\"),\n",
            "+                    make_item(\"⚙️\", \"config\", \"Configure cognee settings\"),\n",
            "+                    id=\"menu-list\",\n",
            "+                    classes=\"menu-list\",\n",
            "+                )\n",
            "+\n",
            "+    def compose_footer(self) -> ComposeResult:\n",
            "+        yield Static(\"↑↓: Navigate  •  Enter: Select  •  q/Esc: Quit\", classes=\"tui-footer\")\n",
            "+\n",
            "+    def on_mount(self) -> None:\n",
            "+        \"\"\"Focus the list view on mount.\"\"\"\n",
            "+        self.lv = self.query_one(ListView)\n",
            "+        self.current_index = 0\n",
            "+        self.set_focus(self.lv)\n",
            "+        self._apply_highlight()\n",
            "+\n",
            "+    def _apply_highlight(self) -> None:\n",
            "+        lv = self.lv\n",
            "+        children = list(lv.children)\n",
            "+        self.lv.index = self.current_index\n",
            "+        for idx, item in enumerate(children):\n",
            "+            if idx == self.current_index:\n",
            "+                item.add_class(\"highlighted\")\n",
            "+            else:\n",
            "+                item.remove_class(\"highlighted\")\n",
            "+\n",
            "+    def action_nav_up(self) -> None:\n",
            "+        self.current_index = max(0, self.current_index - 1)\n",
            "+        self._apply_highlight()\n",
            "+\n",
            "+    def action_nav_down(self) -> None:\n",
            "+        children = list(self.lv.children)\n",
            "+        self.current_index = min(len(children) - 1, self.current_index + 1)\n",
            "+        self._apply_highlight()\n",
            "+\n",
            "+    def on_list_view_selected(self, event: ListView.Selected) -> None:\n",
            "+        selected_index = event.index\n",
            "+        self.current_index = selected_index\n",
            "+        self._apply_highlight()\n",
            "+        if selected_index == 0:  # add\n",
            "+            self.app.push_screen(AddTUIScreen())\n",
            "+        elif selected_index == 1:  # search\n",
            "+            self.app.push_screen(SearchTUIScreen())\n",
            "+        elif selected_index == 2:  # cognify\n",
            "+            self.app.push_screen(CognifyTUIScreen())\n",
            "+        elif selected_index == 3:  # delete\n",
            "+            self.app.push_screen(DeleteTUIScreen())\n",
            "+        elif selected_index == 4:  # config\n",
            "+            self.app.push_screen(ConfigTUIScreen())\n",
            "+        else:\n",
            "+            self.app.exit()\n",
            "+\n",
            "+    def action_select(self) -> None:\n",
            "+        \"\"\"Select the current item.\"\"\"\n",
            "+        list_view = self.query_one(ListView)\n",
            "+        list_view.action_select_cursor()\n",
            "+\n",
            "+    def action_quit_app(self) -> None:\n",
            "+        \"\"\"Quit the entire application.\"\"\"\n",
            "+        self.app.exit()\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE: cognee/cli/tui/search_screen.py | status=added | +179 -0 | changes=179\n",
            "PATCH:\n",
            "@@ -0,0 +1,179 @@\n",
            "+import asyncio\n",
            "+from textual.app import ComposeResult\n",
            "+from textual.widgets import Input, Label, Static, Select\n",
            "+from textual.containers import Container, Vertical, ScrollableContainer\n",
            "+from textual.binding import Binding\n",
            "+from cognee.cli.tui.base_screen import BaseTUIScreen\n",
            "+\n",
            "+\n",
            "+class SearchTUIScreen(BaseTUIScreen):\n",
            "+    \"\"\"Simple search screen with query input and results display.\"\"\"\n",
            "+\n",
            "+    BINDINGS = [\n",
            "+        Binding(\"q\", \"quit_app\", \"Quit\"),\n",
            "+        Binding(\"escape\", \"back\", \"Back\"),\n",
            "+        Binding(\"ctrl+s\", \"search\", \"Search\"),\n",
            "+    ]\n",
            "+\n",
            "+    CSS = (\n",
            "+        BaseTUIScreen.CSS\n",
            "+        + \"\"\"\n",
            "+    #search-form {\n",
            "+        height: auto;\n",
            "+        border: solid $primary;\n",
            "+        padding: 1;\n",
            "+        margin-bottom: 1;\n",
            "+    }\n",
            "+\n",
            "+    #search-form Label {\n",
            "+        margin-bottom: 0;\n",
            "+        color: $text-muted;\n",
            "+    }\n",
            "+\n",
            "+    #search-form Input, #search-form Select {\n",
            "+        margin-bottom: 1;\n",
            "+    }\n",
            "+\n",
            "+    #results-container {\n",
            "+        height: 1fr;\n",
            "+        border: solid $primary;\n",
            "+        padding: 1;\n",
            "+    }\n",
            "+\n",
            "+    #results-title {\n",
            "+        text-style: bold;\n",
            "+        color: $accent;\n",
            "+        margin-bottom: 1;\n",
            "+    }\n",
            "+\n",
            "+    #results-content {\n",
            "+        height: 1fr;\n",
            "+        overflow-y: auto;\n",
            "+    }\n",
            "+    \"\"\"\n",
            "+    )\n",
            "+\n",
            "+    def __init__(self):\n",
            "+        super().__init__()\n",
            "+        self.is_searching = False\n",
            "+\n",
            "+    def compose_content(self) -> ComposeResult:\n",
            "+        with Container(classes=\"tui-main-container\"):\n",
            "+            with Container(classes=\"tui-title-wrapper\"):\n",
            "+                yield Static(\"🔍 Search Data\", classes=\"tui-title-bordered\")\n",
            "+            with Vertical(id=\"search-form\"):\n",
            "+                yield Label(\"Query:\", classes=\"tui-label-spaced\")\n",
            "+                yield Input(placeholder=\"Enter your search query...\", id=\"query-input\")\n",
            "+                yield Label(\"Search Type:\", classes=\"tui-label-spaced\")\n",
            "+                yield Select(\n",
            "+                    [\n",
            "+                        (\"Graph Completion (Recommended)\", \"GRAPH_COMPLETION\"),\n",
            "+                        (\"RAG Completion\", \"RAG_COMPLETION\"),\n",
            "+                        (\"Chunks\", \"CHUNKS\"),\n",
            "+                        (\"Summaries\", \"SUMMARIES\"),\n",
            "+                        (\"Coding Rules\", \"CODING_RULES\"),\n",
            "+                    ],\n",
            "+                    value=\"GRAPH_COMPLETION\",\n",
            "+                    id=\"query-type-select\",\n",
            "+                )\n",
            "+            with Container(id=\"results-container\"):\n",
            "+                yield Static(\"Results\", id=\"results-title\")\n",
            "+                with ScrollableContainer(id=\"results-content\"):\n",
            "+                    yield Static(\n",
            "+                        \"Enter a query and click Search to see results.\", id=\"results-text\"\n",
            "+                    )\n",
            "+\n",
            "+    def compose_footer(self) -> ComposeResult:\n",
            "+        yield Static(\"Ctrl+S: Search  •  Esc: Back  •  q: Quit\", classes=\"tui-footer\")\n",
            "+\n",
            "+    def on_mount(self) -> None:\n",
            "+        \"\"\"Focus the query input on mount.\"\"\"\n",
            "+        query_input = self.query_one(\"#query-input\", Input)\n",
            "+        query_input.focus()\n",
            "+\n",
            "+    def action_back(self) -> None:\n",
            "+        \"\"\"Go back to home screen.\"\"\"\n",
            "+        self.app.pop_screen()\n",
            "+\n",
            "+    def action_quit_app(self) -> None:\n",
            "+        \"\"\"Quit the entire application.\"\"\"\n",
            "+        self.app.exit()\n",
            "+\n",
            "+    def action_search(self) -> None:\n",
            "+        \"\"\"Trigger search action.\"\"\"\n",
            "+        if not self.is_searching:\n",
            "+            self._perform_search()\n",
            "+\n",
            "+    def on_input_submitted(self, event: Input.Submitted) -> None:\n",
            "+        \"\"\"Handle Enter key in query input.\"\"\"\n",
            "+        if event.input.id == \"query-input\":\n",
            "+            self._perform_search()\n",
            "+\n",
            "+    def _perform_search(self) -> None:\n",
            "+        \"\"\"Perform the search operation.\"\"\"\n",
            "+        if self.is_searching:\n",
            "+            return\n",
            "+\n",
            "+        query_input = self.query_one(\"#query-input\", Input)\n",
            "+        query_text = query_input.value.strip()\n",
            "+\n",
            "+        if not query_text:\n",
            "+            self.notify(\"Please enter a search query\", severity=\"warning\")\n",
            "+            return\n",
            "+\n",
            "+        query_type_select = self.query_one(\"#query-type-select\", Select)\n",
            "+        query_type = str(query_type_select.value)\n",
            "+\n",
            "+        self.is_searching = True\n",
            "+        self.notify(f\"Searching for: {query_text}\", severity=\"information\")\n",
            "+\n",
            "+        # Update results to show loading\n",
            "+        results_text = self.query_one(\"#results-text\", Static)\n",
            "+        results_text.update(\"🔍 Searching...\")\n",
            "+\n",
            "+        # Run async search\n",
            "+        asyncio.create_task(self._async_search(query_text, query_type))\n",
            "+\n",
            "+    async def _async_search(self, query_text: str, query_type: str) -> None:\n",
            "+        \"\"\"Async search operation.\"\"\"\n",
            "+        try:\n",
            "+            import cognee\n",
            "+            from cognee.modules.search.types import SearchType\n",
            "+\n",
            "+            # Convert string to SearchType enum\n",
            "+            search_type = SearchType[query_type]\n",
            "+            # Perform search\n",
            "+            results = await cognee.search(\n",
            "+                query_text=query_text,\n",
            "+                query_type=search_type,\n",
            "+                system_prompt_path=\"answer_simple_question.txt\",\n",
            "+                top_k=10,\n",
            "+            )\n",
            "+\n",
            "+            # Update results display\n",
            "+            results_text = self.query_one(\"#results-text\", Static)\n",
            "+\n",
            "+            if not results:\n",
            "+                results_text.update(\"No results found for your query.\")\n",
            "+            else:\n",
            "+                # Format results based on type\n",
            "+                if query_type in [\"GRAPH_COMPLETION\", \"RAG_COMPLETION\"]:\n",
            "+                    formatted = \"\\n\\n\".join([f\"📝 {result}\" for result in results])\n",
            "+                elif query_type == \"CHUNKS\":\n",
            "+                    formatted = \"\\n\\n\".join(\n",
            "+                        [f\"📄 Chunk {i + 1}:\\n{result}\" for i, result in enumerate(results)]\n",
            "+                    )\n",
            "+                else:\n",
            "+                    formatted = \"\\n\\n\".join([f\"• {result}\" for result in results])\n",
            "+\n",
            "+                results_text.update(formatted)\n",
            "+\n",
            "+            self.notify(f\"✓ Found {len(results)} result(s)\", severity=\"information\")\n",
            "+\n",
            "+        except Exception as e:\n",
            "+            results_text = self.query_one(\"#results-text\", \n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE: cognee/infrastructure/databases/relational/sqlalchemy/SqlAlchemyAdapter.py | status=modified | +32 -22 | changes=54\n",
            "PATCH:\n",
            "@@ -3,8 +3,7 @@\n",
            " from os import path\n",
            " import tempfile\n",
            " from uuid import UUID\n",
            "-from typing import Optional\n",
            "-from typing import AsyncGenerator, List\n",
            "+from typing import Optional, AsyncGenerator, List, Union\n",
            " from contextlib import asynccontextmanager\n",
            " from sqlalchemy.orm import joinedload\n",
            " from sqlalchemy.exc import NoResultFound\n",
            "@@ -257,35 +256,46 @@ async def get_schema_list(self) -> List[str]:\n",
            "                 return [schema[0] for schema in result.fetchall()]\n",
            "         return []\n",
            " \n",
            "-    async def delete_entity_by_id(\n",
            "-        self, table_name: str, data_id: UUID, schema_name: Optional[str] = \"public\"\n",
            "+    async def delete_entities_by_id(\n",
            "+        self,\n",
            "+        table_name: str,\n",
            "+        data_id: Union[UUID, List[UUID]],  # Supports a single UUID or a List of UUIDs\n",
            "+        schema_name: Optional[str] = \"public\",\n",
            "     ):\n",
            "         \"\"\"\n",
            "-        Delete an entity from the specified table based on its unique ID.\n",
            "+        Delete one or more entities from the specified table based on their ID(s).\n",
            " \n",
            "         Parameters:\n",
            "         -----------\n",
            "-\n",
            "-            - table_name (str): The name of the table from which to delete the entity.\n",
            "-            - data_id (UUID): The unique identifier of the entity to be deleted.\n",
            "-            - schema_name (Optional[str]): The name of the schema where the table resides,\n",
            "-              defaults to 'public'. (default 'public')\n",
            "+            - table_name (str): The name of the table from which to delete the entities.\n",
            "+            - data_id (Union[UUID, List[UUID]]): The unique identifier(s) to be deleted.\n",
            "+            - schema_name (Optional[str]): The name of the schema where the table resides.\n",
            "         \"\"\"\n",
            "-        if self.engine.dialect.name == \"sqlite\":\n",
            "-            async with self.get_async_session() as session:\n",
            "-                TableModel = await self.get_table(table_name, schema_name)\n",
            " \n",
            "-                # Foreign key constraints are disabled by default in SQLite (for backwards compatibility),\n",
            "-                # so must be enabled for each database connection/session separately.\n",
            "+        # Ensure data_ids is a list for the WHERE clause logic\n",
            "+        if isinstance(data_id, list):\n",
            "+            data_ids_to_delete = data_id\n",
            "+        else:\n",
            "+            data_ids_to_delete = [data_id]\n",
            "+\n",
            "+        if not data_ids_to_delete:\n",
            "+            return\n",
            "+\n",
            "+        async with self.get_async_session() as session:\n",
            "+            TableModel = await self.get_table(table_name, schema_name)\n",
            "+\n",
            "+            # Handle SQLite's foreign key requirement\n",
            "+            if self.engine.dialect.name == \"sqlite\":\n",
            "+                from sqlalchemy import text\n",
            "+\n",
            "                 await session.execute(text(\"PRAGMA foreign_keys = ON;\"))\n",
            " \n",
            "-                await session.execute(TableModel.delete().where(TableModel.c.id == data_id))\n",
            "-                await session.commit()\n",
            "-        else:\n",
            "-            async with self.get_async_session() as session:\n",
            "-                TableModel = await self.get_table(table_name, schema_name)\n",
            "-                await session.execute(TableModel.delete().where(TableModel.c.id == data_id))\n",
            "-                await session.commit()\n",
            "+            # Construct the DELETE statement using the 'in_()' operator\n",
            "+            stmt = TableModel.delete().where(TableModel.c.id.in_(data_ids_to_delete))\n",
            "+\n",
            "+            # Execute and commit\n",
            "+            await session.execute(stmt)\n",
            "+            await session.commit()\n",
            " \n",
            "     async def delete_data_entity(self, data_id: UUID):\n",
            "         \"\"\"\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE: cognee/modules/data/methods/__init__.py | status=modified | +2 -0 | changes=2\n",
            "PATCH:\n",
            "@@ -16,6 +16,8 @@\n",
            " \n",
            " # Delete\n",
            " from .delete_dataset import delete_dataset\n",
            "+from .delete_dataset_by_name import delete_dataset_by_name\n",
            "+from .delete_data_by_user import delete_data_by_user\n",
            " from .delete_data import delete_data\n",
            " \n",
            " # Create\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE: cognee/modules/data/methods/delete_data_by_user.py | status=added | +33 -0 | changes=33\n",
            "PATCH:\n",
            "@@ -0,0 +1,33 @@\n",
            "+from uuid import UUID\n",
            "+from sqlalchemy import select\n",
            "+from cognee.infrastructure.databases.relational import get_relational_engine\n",
            "+from cognee.modules.data.models import Dataset\n",
            "+from cognee.modules.users.methods import get_user\n",
            "+from cognee.shared.logging_utils import get_logger\n",
            "+\n",
            "+logger = get_logger()\n",
            "+\n",
            "+\n",
            "+async def delete_data_by_user(user_id: UUID):\n",
            "+    \"\"\"\n",
            "+    Delete all datasets and their associated data for a specific user.\n",
            "+\n",
            "+    This function performs a comprehensive deletion of all data owned by a user,\n",
            "+    including datasets, data entries, and all related records in the database.\n",
            "+\n",
            "+    Args:\n",
            "+        user_id: UUID of the user whose data should be deleted\n",
            "+\n",
            "+    Raises:\n",
            "+        EntityNotFoundError: If user is not found\n",
            "+    \"\"\"\n",
            "+    db_engine = get_relational_engine()\n",
            "+\n",
            "+    async with db_engine.get_async_session() as session:\n",
            "+        # Verify user exists\n",
            "+        await get_user(user_id)\n",
            "+        # Get all datasets owned by this user\n",
            "+        datasets_query = select(Dataset.id).where(Dataset.owner_id == user_id)\n",
            "+        user_datasets_ids = (await session.execute(datasets_query)).scalars().all()\n",
            "+    if user_datasets_ids:\n",
            "+        await db_engine.delete_entities_by_id(Dataset.__table__.name, user_datasets_ids)\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE: cognee/modules/data/methods/delete_dataset_by_name.py | status=added | +28 -0 | changes=28\n",
            "PATCH:\n",
            "@@ -0,0 +1,28 @@\n",
            "+from uuid import UUID\n",
            "+from sqlalchemy import select\n",
            "+from cognee.infrastructure.databases.relational import get_relational_engine\n",
            "+from ..models import Dataset\n",
            "+\n",
            "+\n",
            "+async def delete_dataset_by_name(dataset_name: str, user_id: UUID):\n",
            "+    \"\"\"\n",
            "+    Delete a single dataset by name for a specific user.\n",
            "+\n",
            "+    Args:\n",
            "+        dataset_name: The name of the dataset to delete (must be a single string).\n",
            "+        user_id: UUID of the dataset owner.\n",
            "+\n",
            "+    \"\"\"\n",
            "+    db_engine = get_relational_engine()\n",
            "+\n",
            "+    async with db_engine.get_async_session() as session:\n",
            "+        dataset_id = (\n",
            "+            await session.scalars(\n",
            "+                select(Dataset.id)\n",
            "+                .filter(Dataset.owner_id == user_id)\n",
            "+                .filter(Dataset.name == dataset_name)\n",
            "+            )\n",
            "+        ).first()\n",
            "+    # Keeping this out of the first session, since delete_entities_by_id creates another session.\n",
            "+    if dataset_id:\n",
            "+        await db_engine.delete_entities_by_id(Dataset.__table__.name, dataset_id)\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE: cognee/shared/logging_utils.py | status=modified | +3 -2 | changes=5\n",
            "PATCH:\n",
            "@@ -285,7 +285,7 @@ def cleanup_old_logs(logs_dir, max_files):\n",
            "         return False\n",
            " \n",
            " \n",
            "-def setup_logging(log_level=None, name=None):\n",
            "+def setup_logging(log_level=None, name=None, enable_console_logging=True):\n",
            "     \"\"\"Sets up the logging configuration with structlog integration.\n",
            " \n",
            "     Args:\n",
            "@@ -465,7 +465,8 @@ def emit(self, record):\n",
            "     root_logger = logging.getLogger()\n",
            "     if root_logger.hasHandlers():\n",
            "         root_logger.handlers.clear()\n",
            "-    root_logger.addHandler(stream_handler)\n",
            "+    if enable_console_logging:\n",
            "+        root_logger.addHandler(stream_handler)\n",
            " \n",
            "     # Note: root logger needs to be set at NOTSET to allow all messages through and specific stream and file handlers\n",
            "     # can define their own levels.\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE: pyproject.toml | status=modified | +1 -0 | changes=1\n",
            "PATCH:\n",
            "@@ -156,6 +156,7 @@ redis = [\"redis>=5.0.3,<6.0.0\"]\n",
            " monitoring = [\"sentry-sdk[fastapi]>=2.9.0,<3\", \"langfuse>=2.32.0,<3\"]\n",
            " \n",
            " docling = [\"docling>=2.54\", \"transformers>=4.55\"]\n",
            "+textual = [\"textual>=6.6.0\"]\n",
            " \n",
            " [project.urls]\n",
            " Homepage = \"https://www.cognee.ai\"\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE: uv.lock | status=modified | +60 -1 | changes=61\n",
            "PATCH:\n",
            "@@ -1100,6 +1100,9 @@ scraping = [\n",
            "     { name = \"protego\" },\n",
            "     { name = \"tavily-python\" },\n",
            " ]\n",
            "+textual = [\n",
            "+    { name = \"textual\" },\n",
            "+]\n",
            " \n",
            " [package.metadata]\n",
            " requires-dist = [\n",
            "@@ -1197,6 +1200,7 @@ requires-dist = [\n",
            "     { name = \"structlog\", specifier = \">=25.2.0,<26\" },\n",
            "     { name = \"tavily-python\", marker = \"extra == 'scraping'\", specifier = \">=0.7.12\" },\n",
            "     { name = \"tenacity\", specifier = \">=9.0.0\" },\n",
            "+    { name = \"textual\", marker = \"extra == 'textual'\", specifier = \">=6.6.0\" },\n",
            "     { name = \"tiktoken\", specifier = \">=0.8.0,<1.0.0\" },\n",
            "     { name = \"transformers\", marker = \"extra == 'codegraph'\", specifier = \">=4.46.3,<5\" },\n",
            "     { name = \"transformers\", marker = \"extra == 'docling'\", specifier = \">=4.55\" },\n",
            "@@ -1210,7 +1214,7 @@ requires-dist = [\n",
            "     { name = \"uvicorn\", specifier = \">=0.34.0,<1.0.0\" },\n",
            "     { name = \"websockets\", specifier = \">=15.0.1,<16.0.0\" },\n",
            " ]\n",
            "-provides-extras = [\"api\", \"distributed\", \"scraping\", \"neo4j\", \"neptune\", \"postgres\", \"postgres-binary\", \"notebook\", \"langchain\", \"llama-index\", \"huggingface\", \"ollama\", \"mistral\", \"anthropic\", \"deepeval\", \"posthog\", \"groq\", \"chromadb\", \"docs\", \"codegraph\", \"evals\", \"graphiti\", \"aws\", \"dlt\", \"baml\", \"dev\", \"debug\", \"redis\", \"monitoring\", \"docling\"]\n",
            "+provides-extras = [\"api\", \"distributed\", \"scraping\", \"neo4j\", \"neptune\", \"postgres\", \"postgres-binary\", \"notebook\", \"langchain\", \"llama-index\", \"huggingface\", \"ollama\", \"mistral\", \"anthropic\", \"deepeval\", \"posthog\", \"groq\", \"chromadb\", \"docs\", \"codegraph\", \"evals\", \"graphiti\", \"aws\", \"dlt\", \"baml\", \"dev\", \"debug\", \"redis\", \"monitoring\", \"docling\", \"textual\"]\n",
            " \n",
            " [[package]]\n",
            " name = \"colorama\"\n",
            "@@ -3807,6 +3811,18 @@ wheels = [\n",
            "     { url = \"https://files.pythonhosted.org/packages/6d/c9/556846b9d112a3387397850d5560f5ec63464508c6aa068257f0516159d0/limits-4.8.0-py3-none-any.whl\", hash = \"sha256:de43d24969a0050b859dd29bbd61bd807a5de3ed9255f666aec1ea3dd3fc407e\", size = 62028, upload-time = \"2025-04-23T21:00:26.017Z\" },\n",
            " ]\n",
            " \n",
            "+[[package]]\n",
            "+name = \"linkify-it-py\"\n",
            "+version = \"2.0.3\"\n",
            "+source = { registry = \"https://pypi.org/simple\" }\n",
            "+dependencies = [\n",
            "+    { name = \"uc-micro-py\" },\n",
            "+]\n",
            "+sdist = { url = \"https://files.pythonhosted.org/packages/2a/ae/bb56c6828e4797ba5a4821eec7c43b8bf40f69cda4d4f5f8c8a2810ec96a/linkify-it-py-2.0.3.tar.gz\", hash = \"sha256:68cda27e162e9215c17d786649d1da0021a451bdc436ef9e0fa0ba5234b9b048\", size = 27946, upload-time = \"2024-02-04T14:48:04.179Z\" }\n",
            "+wheels = [\n",
            "+    { url = \"https://files.pythonhosted.org/packages/04/1e/b832de447dee8b582cac175871d2f6c3d5077cc56d5575cadba1fd1cccfa/linkify_it_py-2.0.3-py3-none-any.whl\", hash = \"sha256:6bcbc417b0ac14323382aef5c5192c0075bf8a9d6b41820a2b66371eac6b6d79\", size = 19820, upload-time = \"2024-02-04T14:48:02.496Z\" },\n",
            "+]\n",
            "+\n",
            " [[package]]\n",
            " name = \"litellm\"\n",
            " version = \"1.80.0\"\n",
            "@@ -4139,6 +4155,11 @@ wheels = [\n",
            "     { url = \"https://files.pythonhosted.org/packages/94/54/e7d793b573f298e1c9013b8c4dade17d481164aa517d1d7148619c2cedbf/markdown_it_py-4.0.0-py3-none-any.whl\", hash = \"sha256:87327c59b172c5011896038353a81343b6754500a08cd7a4973bb48c6d578147\", size = 87321, upload-time = \"2025-08-11T12:57:51.923Z\" },\n",
            " ]\n",
            " \n",
            "+[package.optional-dependencies]\n",
            "+linkify = [\n",
            "+    { name = \"linkify-it-py\" },\n",
            "+]\n",
            "+\n",
            " [[package]]\n",
            " name = \"marko\"\n",
            " version = \"2.2.1\"\n",
            "@@ -4305,6 +4326,18 @@ wheels = [\n",
            "     { url = \"https://files.pythonhosted.org/packages/27/1a/1f68f9ba0c207934b35b86a8ca3aad8395a3d6dd7921c0686e23853ff5a9/mccabe-0.7.0-py2.py3-none-any.whl\", hash = \"sha256:6c2d30ab6be0e4a46919781807b4f0d834ebdd6c6e3dca0bda5a15f863427b6e\", size = 7350, upload-time = \"2022-01-24T01:14:49.62Z\" },\n",
            " ]\n",
            " \n",
            "+[[package]]\n",
            "+name = \"mdit-py-plugins\"\n",
            "+version = \"0.5.0\"\n",
            "+source = { registry = \"https://pypi.org/simple\" }\n",
            "+dependencies = [\n",
            "+    { name = \"markdown-it-py\" },\n",
            "+]\n",
            "+sdist = { url = \"https://files.pythonhosted.org/packages/b2/fd/a756d36c0bfba5f6e39a1cdbdbfdd448dc02692467d83816dff4592a1ebc/mdit_py_plugins-0.5.0.tar.gz\", hash = \"sha256:f4918cb50119f50446560513a8e311d574ff6aaed72606ddae6d35716fe809c6\", size = 44655, upload-time = \"2025-08-11T07:25:49.083Z\" }\n",
            "+wheels = [\n",
            "+    { url = \"https://files.pythonhosted.org/packages/fb/86/dd6e5db36df29e76c7a7699123569a4a18c1623ce68d826ed96c62643cae/mdit_py_plugins-0.5.0-py3-none-any.whl\", hash = \"sha256:07a08422fc1936a5d26d146759e9155ea466e842f5ab2f7d2266dd084c8dab1f\", size = 57205, upload-time = \"2025-08-11T07:25:47.597Z\" },\n",
            "+]\n",
            "+\n",
            " [[package]]\n",
            " name = \"mdurl\"\n",
            " version = \"0.1.2\"\n",
            "@@ -8578,6 +8611,23 @@ wheels = [\n",
            "     { url = \"https://files.pythonhosted.org/packages/6a/9e/2064975477fdc887e47ad42157e214526dcad8f317a948dee17e1659a62f/terminado-0.18.1-py3-none-any.whl\", hash = \"sha256:a4468e1b37bb318f8a86514f65814e1afc977cf29b3992a4500d9dd305dcceb0\", size = 14154, upload-time = \"2024-03-12T14:34:36.569Z\" },\n",
            " ]\n",
            " \n",
            "+[[package]]\n",
            "+name = \"textual\"\n",
            "+version = \"6.6.0\"\n",
            "+source = { registry = \"https://pypi.org/simple\" }\n",
            "+dependencies = [\n",
            "+    { name = \"markdown-it-py\", extra = [\"linkify\"] },\n",
            "+    { name = \"mdit-py-plugins\" },\n",
            "+    { name = \"platformdirs\" },\n",
            "+    { name = \"pygments\" },\n",
            "+    { name = \"rich\" },\n",
            "+    { name = \"typing-extensions\" },\n",
            "+]\n",
            "+sdist = { url = \"https://files.pythonhosted.org/packages/f6/2f/f0b408f227edca21d1996c1cd0b65309f0cbff44264aa40aded3ff9ce2e1/textual-6.6.0.tar.gz\", hash = \"sha256:53345166d6b0f9fd028ed0217d73b8f47c3a26679a18ba3b67616dcacb470eec\", size = 1579327, upload-time = \"2025-11-10T17:50:00.038Z\" }\n",
            "+wheels = [\n",
            "+    { url = \"https://files.pythonhosted.org/packages/53/b3/95ab646b0c908823d71e49ab8b5949ec9f33346cee3897d1af6be28a8d91/textual-6.6.0-py3-none-any.whl\", hash = \"sha256:5a9484bd15ee8a6fd8ac4ed4849fb25ee56bed2cecc7b8a83c4cd7d5f19515e5\", size = 712606, upload-time = \"2025-11-10T17:49:58.391Z\" },\n",
            "+]\n",
            "+\n",
            " [[package]]\n",
            " name = \"threadpoolctl\"\n",
            " version = \"3.6.0\"\n",
            "@@ -9112,6 +9162,15 @@ wheels = [\n",
            "     { url = \"https://files.pythonhosted.org/packages/c2/14/e2a54fabd4f08cd7af1c07030603c3356b74da07f7cc056e600436edfa17/tzlocal-5.3.1-py3-none-any.whl\", hash = \"sha256:eb1a66c3ef5847adf7a834f1be0800581b683b5608\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(bundle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Finding(severity='P1', category='correctness', file='cognee/cli/tui/cognify_screen.py', line_range='144-169', title='Incomplete code in _cognify_async method', description=\"The _cognify_async method in CognifyTUIScreen ends abruptly with an incomplete line 'background_checkbox = sel', which is a syntax error and will cause the TUI to fail when cognify is executed.\", recommendation='Complete the method implementation by properly re-enabling the inputs and removing the incomplete line. Ensure the method ends with correct syntax and logic to re-enable UI elements after processing.', confidence=0.9),\n",
              " Finding(severity='P1', category='correctness', file='cognee/cli/tui/delete_screen.py', line_range='211-247', title='Incomplete code in _handle_delete_all method', description=\"The _handle_delete_all method in DeleteTUIScreen ends abruptly with an incomplete line 'se', which is a syntax error and will cause the TUI to fail when attempting to delete all data.\", recommendation='Complete the method implementation by properly handling the confirmation dialog and deletion logic. Remove the incomplete line and ensure the method has correct syntax and logic to confirm and perform delete all operation.', confidence=0.9),\n",
              " Finding(severity='P1', category='correctness', file='cognee/cli/tui/config_screen.py', line_range='97-104', title='Incomplete code in _load_table_data method', description=\"The _load_table_data method in ConfigTUIScreen is truncated and ends abruptly with 'for key, (_, default_val) in self.CONFI', indicating incomplete code. This will cause runtime errors when loading configuration data in the TUI.\", recommendation='Complete the method implementation to fully load configuration data into the table. Ensure the loop and all related logic are properly implemented and the method ends correctly.', confidence=0.9),\n",
              " Finding(severity='P1', category='security', file='cognee/cli/tui/config_screen.py', line_range='entire file', title='Potential Exposure of Sensitive Configuration Data in TUI Config Screen', description=\"The TUI config screen allows inline editing of configuration keys including sensitive keys such as 'llm_api_key', 'vector_db_key', and 'llm_endpoint'. These keys may contain secrets or credentials. Displaying and editing these values in a TUI without masking or additional protection risks accidental exposure of secrets to shoulder surfing or logs.\", recommendation='Implement masking or obfuscation for sensitive configuration values in the TUI. Ensure that sensitive values are not logged or displayed in plain text. Consider adding authentication or authorization checks before allowing access to sensitive configuration editing.', confidence=0.9),\n",
              " Finding(severity='P2', category='security', file='cognee/cli/tui/delete_screen.py', line_range='entire file', title='Lack of Confirmation for Bulk Delete Operations in TUI Delete Screen', description=\"The delete screen allows deletion of datasets or all user data with minimal confirmation. The 'Delete All' operation can be triggered via Ctrl+A or a button, and while there is a mention of confirmation, the code snippet is incomplete and does not clearly show robust confirmation or authorization checks before destructive actions.\", recommendation=\"Ensure that destructive operations like 'Delete All' require explicit user confirmation and proper authorization. Implement confirmation dialogs and possibly require re-authentication or additional safeguards to prevent accidental or unauthorized data deletion.\", confidence=0.8),\n",
              " Finding(severity='P2', category='security', file='cognee/infrastructure/databases/relational/sqlalchemy/SqlAlchemyAdapter.py', line_range='256-310', title='Improper Handling of Deletion Inputs in delete_entities_by_id Method', description='The method delete_entities_by_id accepts a single UUID or a list of UUIDs for deletion. It converts single UUIDs to a list internally. However, if an empty list is passed, it returns early without error. There is no explicit validation of UUID formats or authorization checks before deletion. This could lead to unintended deletions if inputs are not properly validated upstream.', recommendation='Add input validation to ensure that all UUIDs are valid before deletion. Implement authorization checks to ensure the caller has permission to delete the specified entities. Consider logging deletion operations for audit purposes.', confidence=0.7)]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "findings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'@@ -92,6 +92,7 @@ def _discover_commands() -> List[Type[SupportsCliCommand]]:\\n         (\"cognee.cli.commands.cognify_command\", \"CognifyCommand\"),\\n         (\"cognee.cli.commands.delete_command\", \"DeleteCommand\"),\\n         (\"cognee.cli.commands.config_command\", \"ConfigCommand\"),\\n+        (\"cognee.cli.commands.tui_command\", \"TuiCommand\"),\\n     ]\\n \\n     for module_path, class_name in command_modules:'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "files[0].patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
